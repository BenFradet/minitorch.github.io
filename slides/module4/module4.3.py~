# + slideshow={"slide_type": "skip"}
from mt_diagrams.drawing import connect
from mt_diagrams.tensor_draw import color, draw_equation, matrix, matrix_op, t, image_matmul_full, image_matmul_simple, m
import chalk
chalk.set_svg_draw_height(400)
chalk.set_svg_height(200)
import numba
import minitorch

# + [markdown] slideshow={"slide_type": "slide"}
# Module 4.3 - Advanced NNs
# ===================================

# + [markdown] slideshow={"slide_type": "slide"}
# "Pooling"
# -----------
# Reduction applied to each region:

# ![](https://minitorch.github.io/figs/Conv/pool2.png)
#            

# + [markdown] slideshow={"slide_type": "slide"}
# Simple Implementation
# ----------------------

# * Ensure that it is contiguous
# * Use View to "fold" the tensor
#
# ![](https://minitorch.github.io/figs/Conv/pool3.png)

# + [markdown] slideshow={"slide_type": "slide"}
# Why does folding work?
# -----------------------
# * View requires "contiguous" tensor
# * View(4, 2) makes strides (2, 1)
# ![](https://minitorch.github.io/figs/Conv/pool3.png)


# + [markdown] slideshow={"slide_type": "slide"}
# Simple Implementation
# ----------------------

# * Reduce along created fold
# ![](https://minitorch.github.io/figs/Conv/pool4.png)


# + [markdown] slideshow={"slide_type": "slide"}
# Quiz
# ----------------

# + [markdown] slideshow={"slide_type": "slide"}
# Gradient Flow
# --------------

# * Layers that are used get more updates
# * Gradient signals which aspect was important
# * Can have extra layers


# + [markdown] slideshow={"slide_type": "slide"}
# More Reductions
# ----------------
# * Heading for a `max` reduction
# * Heading for a `softmax` output
# * Quick detour

# + [markdown] slideshow={"slide_type": "slide"}
# ReLU, Step, Sigmoid
# ====================

# + [markdown] slideshow={"slide_type": "slide"}
# Basic Operations
# -----------------
# * Introduced in Module-0
# * Widely used in ML
# * What is it?

# + [markdown] slideshow={"slide_type": "slide"}
# Simple Function: ReLU
# ------------------------

# Main "activation" function

# ![](https://minitorch.github.io/figs/Graphs/relu2.png)

# Primarily used to split the data.

# + [markdown] slideshow={"slide_type": "slide"}
# Simple Function: Step
# --------------------------

# Step function $f(x) = x > 0$ determines correct answer

# ![](https://minitorch.github.io/figs/step.png)


# + [markdown] slideshow={"slide_type": "slide"}
# ReLU
# -------------
# Mathematically,

#  $$\text{ReLU}(x) = \max\{0, x\}$$

# Simplest `max` function.

# + [markdown] slideshow={"slide_type": "slide"}
# Step
# ------
# Mathematically,
#
#  $$\text{step}(x) = x > 0 = \arg\max\{0, x\}$$
#
# Simplest `argmax` function.


# + [markdown] slideshow={"slide_type": "slide"}
# Relationship
# -------------

# Step is derivative of ReLU

#  $$
#    \begin{eqnarray*}
#    \text{ReLU}'(x) &=& \begin{cases} 0 & \text{if } x \leq 0 \\ 1 & \text{ow}  \end{cases} \\
#    \text{step}(x) &=& \text{ReLU}'(x)
#    \end{eqnarray*}
#  $$

# + [markdown] slideshow={"slide_type": "slide"}
# What's wrong with step?
# ------------------------

# ![](https://minitorch.github.io/figs/Graphs/incorrect.png)
#          

# Loss of step tells us how many points are wrong.

# + [markdown] slideshow={"slide_type": "slide"}
# Derivative of Step?
# --------------------

# Mathematically,

#  $$\text{step}'(x) = \begin{cases} 0 & \text{if } x \leq 0 \\ 0 & \text{ow}  \end{cases}$$


# Not a useful function to differentiate

# + [markdown] slideshow={"slide_type": "slide"}
# Altenative Function: Sigmoid
# -----------------------------

# Used to determine the loss function

# ![](https://minitorch.github.io/figs/Graphs/sigmoid.png)
#         

# + [markdown] slideshow={"slide_type": "slide"}
# Sigmoid acts as a "soft" version
# --------------------------------

# ![](https://minitorch.github.io/figs/Graphs/distance.png)
#            

# + [markdown] slideshow={"slide_type": "slide"}
# Soft (arg)max?
# --------------------
# Would be nice to have a version that with a useful derivative

#  $$\text{sigmoid}(x) = \text{softmax} \{0, x\}$$


# Useful soft version of argmax.


# + [markdown] slideshow={"slide_type": "slide"}
# Max, Argmax, Softmax
# ====================

# + [markdown] slideshow={"slide_type": "slide"}
# Challenge
# ---------
# How do we generalize sigmoid to multiple outputs?

# ![](https://minitorch.github.io/figs/Conv/value.png)


# + [markdown] slideshow={"slide_type": "slide"}
# Max reduction
# --------------
# * Max is a binary associative operator

# * $\max(a, b)$ returns max value

# * Generalized $\text{ReLU}(a) = \max(a, 0)$

# + [markdown] slideshow={"slide_type": "slide"}
# Max Pooling
# -----------
# * Common to apply pooling with max
# * Sets pooled value to "most active" in block
# * Forward code is easy to implement

# + [markdown] slideshow={"slide_type": "slide"}
# Max Backward
# -------------
# * Unlike sum, max throws away other values
# * Only top value gets used
# * Backward needs to know this.

# + [markdown] slideshow={"slide_type": "slide"}
# Argmax
# ------
# * Function that returns `argmax`, one-hot
# * Generalizes step

# ![](https://minitorch.github.io/figs/Conv/argmax.png)


# + [markdown] slideshow={"slide_type": "slide"}
# Max Backward
# ----------------

# * First compute `argmax`
# * Only send gradient to `argmax` gradinput
# * Everything else is 0

# + [markdown] slideshow={"slide_type": "slide"}
# Ties
# -----
# * What if there are two or more argmax's?
# * Max is non-differentiable, like `ReLU(0)`.
# * Short answer: Ignore, pick one

# + [markdown] slideshow={"slide_type": "slide"}
# HW
# ----

# * When writing tests for max, ties will break finite-differences
# * Suggestion: perturb your input by adding a small amount of random noise.

# + [markdown] slideshow={"slide_type": "slide"}
# Soft argmax?
# -------------
# * Need a soft version of argmax.
# * Generalizes sigmoid for our new loss function
# * Standard name -> softmax

# + [markdown] slideshow={"slide_type": "slide"}
# Softmax
# -------

#  $$\text{softmax}(\textbf{x}) = \frac{\exp \textbf{x}}{\sum_i \exp x_i}$$

# + [markdown] slideshow={"slide_type": "slide"}
# Sigmoid is Softmax
# -------------------
#  $$\text{softmax}([0, x])[1] = \frac{\exp x}{\exp x + \exp 0} = \sigma(x)$$

# + [markdown] slideshow={"slide_type": "slide"}
# Softmax
# ---------
# ![](https://minitorch.github.io/figs/Conv/softmax.png)

# + [markdown] slideshow={"slide_type": "slide"}
# Review
# -------
# * ReLU   ->  Max
# * Step   ->  Argmax
# * Sigmoid ->  Softmax

# + [markdown] slideshow={"slide_type": "slide"}
# Softmax
# =========

# + [markdown] slideshow={"slide_type": "slide"}
# Network
# -------------

# ![](https://minitorch.github.io/figs/Conv/networkcnn.png)

# + [markdown] slideshow={"slide_type": "slide"}
# Softmax Layer
# -------------

# * Produces a probability distribution over outputs (Sum to 1)
# * Derivative similar to sigmoid
# * Lots of interesting practical properties

# + [markdown] slideshow={"slide_type": "slide"}
# Softmax in Context
# -------------------
# * Not a map!
# * Gradient spreads out from one point to all.

# + [markdown] slideshow={"slide_type": "slide"}
# Softmax
# -------
# * (Colab)[https://colab.research.google.com/drive/1EB7MI_3gzAR1gFwPPO27YU9uYzE_odSu]

# + [markdown] slideshow={"slide_type": "slide"}
# Soft Gates
# ============

# + [markdown] slideshow={"slide_type": "slide"}
# New Methods
# ----------------

# * Sigmoid and softmax produce distributions
# * Can be used to "control" information flow

# + [markdown] slideshow={"slide_type": "slide"}
# Example
# -------
# Returns a combination of x and y
#  $$f(x, y, r) = x * \sigma(r) + y * (1 - \sigma(r))$$

# + [markdown] slideshow={"slide_type": "slide"}
# Gradient is controlled
# -----------------------

#  $$\begin{eqnarray*}
#    f'_x(x, y, r) &= \sigma(r) \\
#    f'_y(x, y, r) &= (1 - \sigma( r))\\
#    f'_r(x, y, r) &= (x -  y) \sigma'(r)
# \end{eqnarray*}$$

# + [markdown] slideshow={"slide_type": "slide"}
# Neural Network Gates
# -----------------------
# Learn which one of the previous layers is most useful.
#  $$\begin{eqnarray*}
#    r &= NN_1 \\
#    x &= NN_2 \\
#    y &= NN_3\\
#   \end{eqnarray*}
#  $$

# + [markdown] slideshow={"slide_type": "slide"}
# Gradient Flow
# --------------
# * Layers that are used get more updates
# * Gradient signals which aspect was important
# * Can have extra layers

# + [markdown] slideshow={"slide_type": "slide"}
# Selecting Choices
# -------------------
# * Gating gives us a binary choice
# * What if we want to select between many elements?
# * Softmax!

# + [markdown] slideshow={"slide_type": "slide"}
# Softmax Gating
# ---------------
# Combines many elements of X based on R
# 
#  $$f(X, R) = X \times softmax(R)$$

# + [markdown] slideshow={"slide_type": "slide"}
# Softmax Gating
# ---------------

# * Brand name: Attention
#

# + [markdown] slideshow={"slide_type": "slide"}
# Example: Translation
# --------------------
# * Show example

# + [markdown] slideshow={"slide_type": "slide"}
# Example: GPT-3
# --------------------
# * Show example

# + [markdown] slideshow={"slide_type": "slide"}
# QA
# ---
