Assignment 2 - Tensors
********************************

In the last section we developed a full autodifferentiation system
using scalars. This system is fully correct, but as we saw, it is not
very efficient. Each operation required storing a graph of all the
values that we had previously created; training required repeating
these operations; computing modules such as linear required a `for`
loop over each of the terms in our network. In this homework we
introduce and implement as  new `tensor` object that will allow us to
do all of these operations in a single steps.


==========================================
Tasks 0: What is a Tensor?
=========================================

Tensor is a fancy name for a simple concept. A tensor is a
`multi-dimensional array` of arbitray dimensions. For now you should
think of it as a convenient way to hold data instead of python lists
or tuples. The simplest non-trivial tensor is 1-dimensional tensor
(also called a `vector`).


.. image:: vector.png

A 2-dimensional tensor is called a `matrix`

.. image:: matrix.png


In addition to dimension the other critical aspect of a tensor are its
`shape` and `size`. We say the `shape` of the matrix is (4, 3) and its size (number of squares) is 12.

A 3-dimensional tensor looks like this.

.. image:: tensor.png

This has shape () and size .


We access an element of the tensor by tensor index notation, `tensor[i]` for 1-dimension,
`tensor[i, j]` for 2-dimension, `tensor[i, j, k]` for 3-dimension, and so forth.


Typically we will use tensors just like multi-dimensional arrays, but
there are a couple properties that are worth knowing about now because
they are just a little different.

First, is that tensors make it really easy to change the order of the
dimensions. For examples to transpose the dimensions of a matrix. For
a general tensor we will refer to this operation as `permute`. Calling
permute will arbitrarily reoder the original tensor. For example, `permute(1,0)`

.. image:: tensor.png

This makes the original matrix of size (4, 3) become (3, 4). Additionally element `tensor[i, j]`
is now accessed as `tensor[j, i]`

Next, tensors make it really easy to add or remove additional
dimensions. We do this by noting that a matrix of size (4, 3) stores
exactly the same data as a matrix of size (1, 4, 3), we can see that
they are the same size.

.. image:: bigger.png

We would like to easily increase and lower the size of our tensors
without changing the data. We will do this with a `view` function
`view(1, 4, 3)`. Element `tensor[i, j]` is now `tensor[1, i, j]`.


Critically neither of these operations changes anything about the
tensor itself! Both view and permute are `tensor tricks`, operations
that modify how we look at the tensor, but not any of its
data. Another way to say this is that they do not move or copy the
data in any way, but only the external tensor wrapper.


* implement `count`
* implement `size`
* implement `print`

==========================================
Tasks 1: Tensor Data - Indexing
==========================================

To make our code a bit simpler we will seperate out the main `tensor
data` object from the user-facing tensor itself.  Tensor data will
implement a core set of high-level operations and then we will wrap
these with easy user functions. The tensor data object is made up of
three parts. Raw data `storage`, tensor `shape` information, and a
tuple of `strides`.

Storage is where the actual data is kept. It is always a 1-D array of
numbers (for simplicity we will always use doubles for now), of length
`size`. There is really not more to it then that, no matter the
dimensionality or shape of the tensor, the storage is just a long
array. The information about the effective number of dimensions and
their individual size is kept in `shape` which is a tuple with exactly
this information.

In a standard multi-dimensional array this would be enough information
to implement indexing. If we wanted to find `tensor[i, j]` in a
matrix of shape (4, 3) we could simply look it up at position `3* i + j`.

.. image:: bigger.png


We refer to this as a `contiguous` tensor.

Things get more interesting if we transponse dimensions. If we first
transpose the tensor (remember, no copy!) and then lookup `tensor[j,
i]` in this way we would go to position `4 * j + i` which is now
wrong!

.. image:: bigger.png


This tells us that `shape` is not enough information to track positions.

Instead we track addition `stride` information on the tensor class. Whereas
`shape` is user-facing and tells them the semantic shape of tensor, `strides`
is internal and tells us, the implementers, where data is located.

Strides is a tuple where each value represents the distance between
values of a give dimension. For example, strides `(3, 1)` says that
moving one position in the first-dimension requires moving 3
positions in storage, but that moving one position in the
second-dimension requires moving only 1 position in storage.

.. image:: bigger.png


Given a lookup `tensor[i, j]` we can directly use strides to find
the storage position

`stide0 * i + stide1 * j `


`stide0 * index0 + stide1 * index1 + stide2 * index2 ... `


* implement `index`
* implement `get`
* implment `enumerate`


==========================================
Tasks 2: Tensor Operations
==========================================

Now that we have a tensor class, our plan will be to reimplement all
our mathematical operations on top of this framework. Instead of
adding scalars we will add tensors. The goal is to make this feel
simple and intuitive to users of the library. ::


  # a) Return a new tensor with the same shape as `tensor_a` where
  # each position is the log/exp/negative of the position in `tensor_a`
  tensor_a.log()
  tensor_a.exp()
  -tensor_a
  ...

  # b) Return a new tensor where
  # each position is the sum/mul/sub of the position in `tensor_a` and `tensor_b`
  tensor_a + tensor_b
  tensor_a * tensor_b
  tensor_a - tensor_b
  ...

  # c) Return a new tensor where dim-1 is size 1 and represents
  # the sum/mean over dim-1 in `tensor_a`
  tensor_a.sum(1)
  tensor_a.mean(1)
  ...

However, instead of directly implementing each of these operations
individually, let's be a bit lazy and note the structural
similarities. We can then implement helpers that do most of the work
for us.

Operation a / map: These operations all just touch each of the
positions of the tensor individually. They don't need to deal with
other positions or know anything about the shape or size of the tensor.

.. image:: bigger.png

Operation b / zip: These operations need to pair operations between
neighboring tensors.


.. image:: bigger.png


Operation c / reduce: These operations need to group together cells within a single tensor.

.. image:: bigger.png


* implement `map`
* implement `zip`
* implement `reduce`


The tensor operations from the last section allow you to map, zip, and
reduce tensor data objects together. On top of this foundation we can
build up a similar class to the Scalar class in
assignment 1. We will call this object `Tensor` to distinguish it from
the raw tensor data.

* read the implementatino for neg, add, mul
* implement the other forward operations for tensor. (pass tests)


==========================================
Tasks 3: Gradients and Autograd
==========================================

We have now moved from scalars and derivatives to vector, matrices and
tensors.  In theory, this means `multivariate calculus` and somewhat
scary terminology. However, most of what we actually need to do will
not require complicated terminology or much technical math. This
section will give a primer to the main ideas, building on assignment 1.

The main new term you need to know is `gradient`. A tensor is a
multidimensional array of scalars, a gradient is a multidimensional
array of derivatives for these scalars. ::


  # Assignment 1 notation
  out = f(a, b, c)
  out.backward()
  (a.derivative, b.derivative, c.derivative)


  # Assignment 2 notation
  tensor1 = tensor(a, b, c)
  out = g(tensor1)
  out.backward()

  # shape (3,)
  tensor1.grad

The gradient of `tensor1` is a tensor that holds the derivatives of
each of its values.


Now, if you look online, you will find lots of different notation for
gradients and multivariate terminology. For this assignment, I ask
that you ignore it and stick to everything you know about derivatives.
It turns out that you can do must of machine learning without ever
thinking in higher dimensions.

How could this be true? Well lets look at the operations from the
previous section.

1) `map`. Given a tensor, map applies a fixed operation to each scalar
   position individually. For a scalar :math:`x`, it computes
   :math:`g(x)`.  Therefore, from assignment 1, we know that the
   derivative :math:`f(g(x))` is equal to :math:`g'(x) \times d`. This
   means to compute the gradient, we only need to compute the
   derivative for each position and map.


.. image:: bigger.png

2) `zip`. Given two tensors, zip applies a fixed operation to each
   scalar position individually. For two scalars :math:`x` and
   :math:`y`, it computes :math:`g(x, y)`.  Therefore, from assignment
   1, we know that the derivative :math:`f(g(x, y))` is equal to
   :math:`g_x'(x, y) \times d` and :math:`g_y'(x, y) \times d`. This
   means to compute the gradient, we only need to compute the
   derivative for each position and map.

.. image:: bigger.png

3) `reduce`. Given a tensor, reduce applies a fixed aggregation
   operation to one dimension. For simplicity lets consider sum-based
   reductions.  For scalars :math:`x_1` to :math:`x_n`, it computes
   :math:`x_1 + x_2 + \ldots`.  For any :math:`x` value this
   yields 1. Therefore, the derivative for any position is simply the
   derivative passed backward :math:`d`. This means to compute the
   gradient, we only need to expand the derivative for each
   position. For other reduce operations such as `product`, you get
   different expansions (but these can be calculated just by taking
   derivatives).

.. image:: bigger.png


* read the implementatino for neg, add, mul
* implement the other forward operations for tensor. (pass tests)

EC

* Implement product


==========================================
Tasks 4: Tensor Broadcasting
==========================================

So far all of our `zip` operations have assumed that we had two
tensors of exactly the same size and shape. However there are many
times when it is interesting to `zip` two tensors are different size.


Perhaps the simplest case is when we have one vector (1-D tensor) and want to add a
constant to every position ::

  vector1 + tensor([10])

We would like for this operation to have the standard interpretation
of adding 10 to each position.  We can view the constant as a tensor of
size 1 and shape (1,) and assume `vector1` is of shape (3,).
Because of this, simple `zip` will fail.

A natural fix is to `broadcast` this dimension to automatically grow
to the size of the vector. That is inside zip we "pretend" that 10 is a vector of
size (3,) that repeats its value, i.e. ::

  vector1 + tensor([10, 10, 10])


(Note we never actually create this tensor. This is just an interpretation.)


* Rule 1: Any dimension of size 1, can be zipped with dimensions of size n > 1 by assuming the dimension is copied n times.


Now let's try apply this approach to a matrix of shape (4, 3). ::

    matrix1 + tensor([10])


Here we are trying to zip a matrix (2-D) of shape (4, 3) with a vector
(1-D) of shape (1,).  It seems like this should fail, but remember we
are always allowed to add "empty" (shape-1) dimensions. If we add an
empty dimension and then apply rule 1 twice we can zip the two together. ::

    matrix1 + tensor([10] * 12, shape=(4, 3))

(Note we never actually create this tensor. This is just an interpretation.)


* Rule 2: Any extra dimensions of size 1 can be added to a tensor to ensure that the sizes are the same.


Finally there is a question of where to add the empty dimensions. This
is not an issue in the above case but becomes an issue in more
complicated cases. ::

  # These two lines are equivalent
  matrix1 + vector1
  matrix1 + vector1.view(1, 3)


* Rule 3: Any extra dimensions of size 1 are always added on the left-side of the shape.

This rule has the impact of making the process easy to follow and replicate. You always know what the shape of the final
output will be. For instance, ::


  # This will fail!
  matrix1.permute(1, 0) + vector1


What's happenining here? The issue is that after the `permute` the shape (3, 4) and we are adding a vector of size
(3,). It seems like this should be okay, but the remember rule 3, we can only add new dimensions to the left. If we want to add
right dimensions we need to do it manually.::

  # This will work!
  matrix1.permute(1, 0) + vector1.view(3, 1)


Finally we note that we can use the broadcasting rules and many times as we want in a given setting:  ::

  # This will return shape (7, 2, 3, 5)
  tensor1.view(2, 3, 1) + tensor2.view(7, 2, 1, 5)


*
*
*
