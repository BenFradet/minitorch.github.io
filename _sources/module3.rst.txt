========================
Module 3 - Efficiency
========================

.. image:: figs/gpu/threadid@3x.png
           :align: center


In addition to helping simplify code, tensors provide a basis for
speeding up computation. In fact, they are really the only way to
efficiently write deep learning in a slow language like Python.
However, nothing we have done so far really makes anything faster than
:doc:`module0`. This module is focused on taking advantage of tensors
to write fast code, first on standard CPUs and then using GPUs.


All starter code is available in https://github.com/minitorch/Module-3 .

To begin, remember to first activate your virtual environment.

>>> source venv/bin/activate

And then clone your assignment.

>>> git clone {{STUDENT_ASSIGNMENT3_URL}}
>>> cd Module-3
>>> pip install -Ue .

You will also need the files so be sure to pull them over
to your new repo.

Be sure to continue to follow the :doc:`contributing` guidelines.


.. toctree::
   :maxdepth: 1
   :glob:
   :caption: Guides

   matrixmult
   parallel
   cuda


Tasks
********


Task 3.1: Matrix Multiplication
==================================

.. note:: This task requires basic familiarity with matrix multiplication.
   Be sure to read the section on
   :doc:`matrixmult`.

Matrix multiplication is key to all of the models that we have trained so far.
In the last module, we computed matrix multiplication using broadcasting.
In this task we ask you to implement it directly as a function. Do your best to
make the function efficient, but for now all that matters is that you correctly
produce a multiply function that passes our tests.


.. todo::
   Complete the following function in `minitorch/functions.py` and pass
   tests marked as `task3_1`.

.. autofunction:: minitorch.matrix_multiply

Task 3.2: Parallelization
==================================

.. note:: This task requires basic familiarity with Numba `prange`.
   Be sure to read the section on
   :doc:`parallel` and review :doc:`module2`.


The main backend for our codebase are the three functions `map`, `zip`, and
`reduce`. If
we can speed up these three, everything we built so far will get better. This
exercise
asks you to utilize Numba and the `njit` function to speed up these
functions. In particular
if you can utilize parallelization through `prange` you can get some big
wins. Be careful though!
parallelization can lead to funny bugs.


.. todo::
   Complete the following in `minitorch/fast_ops.py` and pass speed tests
   marked as `task3_2`.


.. autofunction:: minitorch.fast_ops.tensor_map
.. autofunction:: minitorch.fast_ops.tensor_zip
.. autofunction:: minitorch.fast_ops.tensor_reduce


Task 3.3: CUDA Operations
==================================


.. note:: This task requires basic familiarity with CUDA.
   Be sure to read the section on
   :doc:`cuda` and the Numba CUDA guide.

We can do even better than parallelization if we have access to
specialized hardware. This task asks you to build a GPU implementation
of the backend operations. It will be hard to equal what PyTorch does, but
if you are clever you can make these computations really fast.

.. todo::
   Revisit tensor ops and `minitorch/cuda_ops.py` and pass the tests marked as
   `task3_3`.


.. autofunction:: minitorch.cuda_ops.tensor_map
.. autofunction:: minitorch.cuda_ops.tensor_zip
.. autofunction:: minitorch.cuda_ops.tensor_reduce


Task 3.4: Parallel / CUDA Matrix Multiplication
=================================================

.. note:: This task requires basic familiarity with CUDA.
   Be sure to read the section on
   :doc:`cuda` and the Numba CUDA guide.

Finally we can combine both these approaches and implement Numba and CUDA
`matmul`. This operation is probably the most important in all of deep
learning and is central to making models fast. Again, we first strive for
accuracy, but the faster you can make it the better.


.. todo::
   Implement  `minitorch/function.py` with Numba and
   `minitorch/cuda_functions.py` with CUDA and pass tests marked as `task3_4`.

.. autofunction:: minitorch.matmul
.. autofunction:: minitorch.cuda_matmul


Task 3.5: Training
========================

If your code works you should now be able to move on to the tensor
training script in `project/run_fast_tensor.py`.  This code is the same
basic training setup as  :doc:`module2`, but now utilizes your tensor
code. We have left the `matmul` layer blank for you to implement with
your tensor code.


.. todo::
   Implement the missing functions in `project/run_fast_tensor.py`. These
   should
   do exactly the same thing as the corresponding functions in
   `project/run_tensor.py`,
   but now use the faster backend


   Train a tensor model and add your results to the README. Also record the
   time per epoch
   reported by the trainer. (Our parallel implementation gave a 10x speedup).

