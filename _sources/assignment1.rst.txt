Assignment
********************************


Now that preliminaries are done we can really get started.  In this
assignment you will implement a complete basic version of MiniTorch
using scalar values. You will then use this system to train a
mini-model and get your first real results.

To begin, remember to first activate your virtual environment.

>>> source venv/bin/activate

And then clone your assignment.

>>> git clone {{STUDENT_ASSIGNMENT1_URL}}
>>> cd minitorch1

You will also need the files from Assignment 0 so be sure to pull them over
to your new repo.

Be sure to continue to follow the :doc:`contributing` guidelines.


==================================
Task 1.1: Approximate Derivatives
==================================

.. note:: This task requires familiarity with derivative .
   Be sure to first carefully read the section on
   :doc:`derivative` as well as reviewing the basic rules of
   univariate differentiation. 

.. todo::
   Complete the following function in `minitorch/scalar.py` and pass tests marked as `task1_1`.

.. autofunction:: minitorch.scalar.central_difference

                  
========================
Task 1.2: Scalars
========================

.. note:: This task requires familiarity with the scalar class .
   Be sure to first carefully read the section on
   :doc:`scalar` and `Python numerical overrides 
   <https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types/>`_.

Implement the overridden mathematical functions required for the scalar class.
Each of these requires wiring the internal python operator to the correct
:func:`minitorch.Function.forward` call.

.. autofunction:: minitorch.scalar.ScalarFunction.forward
      

.. todo::
   Add calls in `minitorch/scalar.py` for each of the following and pass tests marked as `task1_2`.

.. autofunction:: minitorch.Scalar.__mul__
.. autofunction:: minitorch.scalar.Mul.forward
.. autofunction:: minitorch.Scalar.__add__
.. autofunction:: minitorch.scalar.Add.forward
.. autofunction:: minitorch.Scalar.__lt__
.. autofunction:: minitorch.scalar.LT.forward
.. autofunction:: minitorch.Scalar.__neg__
.. autofunction:: minitorch.scalar.Neg.forward

                  

========================
Task 1.3: Chain-Rule
========================

Now we have a cute way of grouping together function implementation
and implmentations of their symbolic derivatives. We are making
progress, but we still do not have a better method for taking
derivatives of unseen functions. This is where `autodifferentiation`
comes in. It provides a way to chain together individual scalar
functions.



.. autofunction:: minitorch.scalar.ScalarFunction.backward

.. todo::
   Add calls in `minitorch/scalar.py` and `minitorch.FunctionBase` for each of the following, and pass tests marked as `task1_3`.

.. autofunction:: minitorch.scalar.Mul.backward
.. autofunction:: minitorch.scalar.Add.backward
.. autofunction:: minitorch.scalar.LT.backward
.. autofunction:: minitorch.scalar.Neg.backward

.. autofunction:: minitorch.FunctionBase.chain_rule


========================
Task 1.4: Backpropagate
========================

We can also view this as a graph.

.. todo::
   Add calls in `minitorch/autodiff.py` for the following function, and pass tests marked as `task1_4`.

.. autofunction:: minitorch.backpropagate                  
 
========================
Task 1.5: Training
========================
