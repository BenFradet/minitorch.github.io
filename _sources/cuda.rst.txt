========================
GPU Programming
========================


CPU parallelization and operator fusing is important, but when you
really need efficiency and scale, specialized hardware is critical. It
is really hard to exaggerate how important GPU computation is to deep
learning, it makes it possible to run many models that would have been
intractable even just several years ago.

Writing code of GPUs requires a bit more work than our previous
examples, and they have a slightly different programming model than
CPUs, which can take some time to fully understand. Luckily though
there is now a nice Numba library extension that lets us code for
them directly in Python.


Getting Started
----------------

For this assignment you will need to either work on an environment with a
GPU or utilize Google Colab. Google Colab provides free GPUs in a python
notebook
setting. You can change the environment in the menu to request a GPU server

We recommend working in your local setup but then cloning your environment
on to this notebook

>>> !git clone {GITHUB_PATH}

>>> !pip install -r requirements.txt
>>> !pip install -e .


You can then run your tests with the following command.

>>> cd {{PATH}}; git pull origin master; python -m pytest -m test3_3




CUDA
--------

The programming model most commonly used for GPUs in deep learning is
known as CUDA. CUDA is a proprietary extension to C++ for NVidia
devices. Once you get over the terminology though, CUDA is a
relatively straightforward extension to the mathematical code that we
have been writing.

The main mechanism is a `thread`. A thread can run code and store a
small amount of state. We'll represent it as a little Robot.

.. image:: figs/gpu/thread@3x.png
           :align: center
           :width: 200px

Each thread has a tiny amount of fixed local memory it can manipulate.
But it has to be *constant* size.

.. image:: figs/gpu/local\ mem@3x.png
           :align: center
           :width: 400px


Threads hang out together in `blocks`. Think of these like a little
neighborhood.
You can determine the size of the blocks but there are a
lot of restrictions. We'll assume there are less than 32 threads in a
block.

.. image:: figs/gpu/block1d@3x.png
           :align: center

You can also have square or even cube blocks. Here is a square block
where, the length and width of the neighborhood are the block size.

.. image:: figs/gpu/blockdim@3x.png
           :align: center
           :width: 400px


Each thread knows exactly where it is in the block. It gets this information
in local variables telling it the `thread index`.

.. image:: figs/gpu/threadid@3x.png
           :align: center


Threads on the same block can also talk to each other through `shared
memory`.  This is another constant chunk of memory that is associated
with the block and can be accessed and written to by all of these threads.

.. image:: figs/gpu/sharedmem@3x.png


The blocks come together to form a `grid`. Each of the blocks is exactly
the same size and shape
and all have their own shared memory. Each thread also knows its position
in the global grid.


.. image:: figs/gpu/blockid@3x.png

For instance, we can compute the global position for `x, y` as::

  x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
  y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y




Now here is the interesting part. When you write code for CUDA, you have
to code all of the threads with the same code at the same time. Each thread
behaves in lockstep running the same function.


.. image:: figs/gpu/map@3x.png


In NUMBA you can write the thread instructions as a single function . ::

    # Helper function to call in CUDA
    @cuda.jit(device=True)
    def times(a, b):

       return a * b

    # Main cuda launcher
    @cuda.jit()
    def my_func(in, out):
        # Create some local memory
        local = cuda.local.array(5)

        # Find my position.
        x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
        y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y

        # Compute some information
        local[1] = 10

        # Compute some global value
        out[x, y] = times(in[x, y], local[1])


Note though that we cannot call this function directly. We need to `launch`
with
instructions for how to set up the blocks and grid. Here's how you do this
with Numba ::

  threadsperblock = (4, 3)
  blockspergrid = (1, 3)
  my_func[blockspergrid, threadsperblock](in, out)



This sets up a block and grid structure similar to the map function above. the
code in `my_func` is
run simultaneously for all the threads in the structure. However, you have
to be a bit careful as
some of the threads might want to compute values that are outside the memory
of your data structures.::


    # Main cuda launcher
    @cuda.jit()
    def my_func(in, out):
        # Create some local memory
        local = cuda.local.array(5)

        # Find my position.
        x = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x
        y = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y

        # Compute some information
        local[1] = 10

        # Guard some of the threads.
        if x < out.shape[0] and y < out.shape[1]:
             # Compute some global value
             out[x, y] = times(in[x, y], local[1])



