========================
Module 2 - Tensors
========================

We now have a fully developed autodifferentiation system built around
scalars. This system is correct, but it very inefficient. All numbers
are wrap with an object, and each operation requires storing a graph of
all the values that we had previously created. Training requires
repeating these operations, and computing modules such as linear required
a `for` loop over each of the terms in our network.

This module introduces and implements a **tensor** object that will
solve all of these problems. Tensors all of to group together many of
these repeated operations to save python overhead and to pass off
grouped operations to faster implementations.


All starter code is available in https://github.com/minitorch/module2 .

To begin this module, remember to first activate your virtual environment.

>>> source venv/bin/activate

And then clone your assignment.

>>> git clone {{STUDENT_ASSIGNMENT2_URL}}
>>> cd minitorch2

You will need the files from Assignment 0 so be sure to pull them over
to your new repo.

.. toctree::
   :maxdepth: 1
   :glob:
   :caption: Guides

   tensordata
   tensorops
   tensor
   broadcasting

Tasks
********



Tasks 2.1: Tensor Data - Indexing
==========================================

.. note:: This task requires familiarity with tensor indexing.
   Be sure to first carefully read the section on
   :doc:`tensordata`. You may also find it helpful to read
   the tutorials for using tensors/arrays in Torch or Numpy.

The MiniTorch library implements the core multidimensional tensor backend as
:class:`minitorch.TensorData`. This is class handles indexing, storage, transposition,
and low-level details such as strides. Before turning to the user-facing class :class:`minitorch.Tensor`,
first implement these core functions.

.. todo::
   Add calls in `minitorch/tensor_data.py` for each of the following, and pass tests marked as `task2_1`.


.. autofunction:: minitorch.index_to_position
.. autofunction:: minitorch.count
.. autofunction:: minitorch.TensorData.permute




Tasks 2.2: Tensor Operations
==========================================

.. note:: This task requires familiarity with higher-order tensor operations.
   Be sure to first carefully read the section on
   :doc:`tensorops`. You may also find it helpful to go back to
   Module 0 and make sure you understand higher-order functions and currying.


The tensor operations will to apply high-level, higher-order
operations to all values in a tensor simultaneously. In particularly
allowing you to map, zip, and reduce tensor data objects together. On
top of this foundation we can build up a similar class to the Tensor functions
just like we did for the scalar functions. In this task you should first implement
the generic tensor ops and then use these to provide the `forward` functions
for tensors.


.. automodule:: minitorch.tensor_ops

.. todo::
   Add functions in `minitorch/tensor_ops.py` and `minitorch/tensor.py` for each of the following, and pass tests marked as `task2_2`.


.. autofunction:: minitorch.tensor_map
.. autofunction:: minitorch.tensor_zip
.. autofunction:: minitorch.tensor_reduce



.. autofunction:: minitorch.tensor.Mul.forward
.. autofunction:: minitorch.tensor.Sigmoid.forward
.. autofunction:: minitorch.tensor.ReLU.forward
.. autofunction:: minitorch.tensor.Log.forward
.. autofunction:: minitorch.tensor.Mean.forward
.. autofunction:: minitorch.tensor.Copy.forward
.. autofunction:: minitorch.tensor.LT.forward


Tasks 2.3: Gradients and Autograd
==========================================

.. note:: This task requires familiarity with tensor backward operations.
   Be sure to first carefully read the section on
   :doc:`tensor`. You may also find it helpful to go back to
   Module 1 and review `variables` and `functions`.


Just as with :class:`minitorch.Scalar`, the :class:`minitorch.Tensor`
is a Variable that support automatic differentiation. In this task you
will need to implement each of the backward functions and ensure that
they pass the tests.

.. todo::
      Add backward functions `minitorch/tensor_ops.py` for each of the following, and pass tests marked as `task2_3`.


.. autofunction:: minitorch.tensor.Mul.backward
.. autofunction:: minitorch.tensor.Sigmoid.backward
.. autofunction:: minitorch.tensor.ReLU.backward
.. autofunction:: minitorch.tensor.Log.backward
.. autofunction:: minitorch.tensor.Mean.backward
.. autofunction:: minitorch.tensor.Copy.backward
.. autofunction:: minitorch.tensor.LT.backward



Tasks 2.4: Tensor Broadcasting
==========================================

.. note::

   This task requires familiarity with tensor broadcasting.  Be
   sure to first carefully read the section on
   :doc:`broadcasting`. You may also find it helpful to go through
   some of the tutorials of Torch or Numpy broadcasting as it is
   identical.

.. todo::
   Add broadcasing functions to `minitorch/tensor_data.py` and `minitorch/tensor_ops.py` for each of the following, and pass tests marked as `task2_4`.


.. autofunction:: minitorch.shape_broadcast
.. autofunction:: minitorch.broadcast_index_to_position
.. autofunction:: minitorch.tensor_ops.get



Task 2.5: Training
========================

If your code works you should now be able to run the tensor training script in `project/run_tensor.py`.


.. todo::
   Train a tensor model and add you results to the README.

..
   Tasks 2.5: Training
   ==========================================


   * Read project `tensor`
