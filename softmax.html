
<!DOCTYPE html>

<html lang="english">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Softmax &#8212; MiniTorch 0.1 documentation</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/thebelab.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/thebelab-helper.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="Pooling" href="pooling.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="index.html">
<p class="title">MiniTorch</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="setup.html">
  Workspace Setup
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="contributing.html">
  Contributing
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="mlprimer.html">
  ML Primer
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module0.html">
  Module 0 - Fundamentals
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module1.html">
  Module 1 - Auto-Differentiation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module2.html">
  Module 2 - Tensors
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module3.html">
  Module 3 - Efficiency
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="module4.html">
  Module 4 - Networks
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption">
 <span class="caption-text">
  Guides
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="convolution.html">
   Convolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pooling.html">
   Pooling
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Softmax
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sigmoid">
   Sigmoid
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#multiclass">
   Multiclass
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="softmax">
<h1>Softmax<a class="headerlink" href="#softmax" title="Permalink to this headline">¶</a></h1>
<div class="section" id="sigmoid">
<h2>Sigmoid<a class="headerlink" href="#sigmoid" title="Permalink to this headline">¶</a></h2>
<p>So far, the key function that we have relied on for calculating loss
is the <a class="reference external" href="https://en.wikipedia.org/wiki/Sigmoid_function">sigmoid</a>
function.  As visualized below, it goes to zero for large negative
inputs, and goes to 1 for large positive inputs. In between, it forms
a smooth S-curve.</p>
<a class="reference internal image-reference" href="_images/sigmoid.png"><img alt="_images/sigmoid.png" class="align-center" src="_images/sigmoid.png" style="width: 400px;" /></a>
<p>As we saw in Module 1, sigmoid function makes it easier to aply
auto-differentiation when training our models. It can be thought of as
a smooth version of the step function <span class="math notranslate nohighlight">\(x &gt; 0\)</span>, which signals
whether <span class="math notranslate nohighlight">\(x\)</span> is greater than zero by returning a binary value.
Another way to write this step function is <span class="math notranslate nohighlight">\(step(x) = argmax\{0,
x\}\)</span>, i.e. returns which argument is bigger, 0 or 1. Whereas step
function returns a binary choice, sigmoid function gives a &quot;softer&quot;
differentiable choice.</p>
<p>We can connect sigmoid function to another function that we have used
in previous MiniTorch Modules: the ReLU function that we use for
activations. Recall that this function is defined as <span class="math notranslate nohighlight">\(ReLU(x) =
max\{ 0, x\}\)</span> with the following derivative function:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\text{ReLU'}(x) = \begin{cases} 0 &amp; \text{if } x \leq 0 \\ 1 &amp; \text{ow}  \end{cases}\end{split}\]</div>
<a class="reference internal image-reference" href="_images/relu2.png"><img alt="_images/relu2.png" class="align-center" src="_images/relu2.png" style="width: 400px;" /></a>
<p>As a summary, we can connect the above three functions:</p>
<table class="table">
<colgroup>
<col style="width: 26%" />
<col style="width: 74%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Function</p></th>
<th class="head"><p>Comparison (x with 0)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ReLU</p></td>
<td><p>Max</p></td>
</tr>
<tr class="row-odd"><td><p>Step</p></td>
<td><p>Arg Max</p></td>
</tr>
<tr class="row-even"><td><p>Sigmoid</p></td>
<td><p>&quot;Soft&quot; Max</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="multiclass">
<h2>Multiclass<a class="headerlink" href="#multiclass" title="Permalink to this headline">¶</a></h2>
<p>The sigmoid function works great for binary classification problems.
However, for many problems, we may want to do <cite>multiclass</cite>
classification, where we have <span class="math notranslate nohighlight">\(K\)</span> possible output classes to
select from. For these problems, we can assume that the model should
output a <span class="math notranslate nohighlight">\(K\)</span>-dimensional vector which gives a score for each of
the K possible classes:</p>
<img alt="_images/value.png" src="_images/value.png" />
<p>Naturally, we pick the output class that has the highest score. Given a vector,
argmax function returns a one-hot vector with 1 at the position of the
highest-scored element and 0 for all other elements:</p>
<img alt="_images/argmax.png" src="_images/argmax.png" />
<p>While argmax function seems a bit different at the first glance, we
can view it as a generalization of the <span class="math notranslate nohighlight">\(x &gt; 0\)</span> function: each
position is either 0 or 1.  We can also see that its derivative will
be zero almost everywhere: a small perturbation to the input will not
change the output value.</p>
<p>In order to fix this issue, we need a soft version of the argmax
function, just like sigmoid function smooths over the input
changes. The generalization of sigmoid function is appropriately known
as the <cite>softmax</cite> function, which is computed as:</p>
<div class="math notranslate nohighlight">
\[\text{softmax}(\textbf{x}) = \frac{\exp \textbf{x}}{\sum_i \exp x_i}\]</div>
<img alt="_images/softmax.png" src="_images/softmax.png" />
<p>Like the sigmoid function, every value of softmax function is between
0 and 1, and a small change to any of the input scores will result in
a change to all of the output values.</p>
<p>As the softmax function requires exponentiating the input scores, it can be
numerically unstable in practice. Therefore it is common to use a
numerical trick to compute the log of the softmax function instead:</p>
<div class="math notranslate nohighlight">
\[\text{logsoftmax}(\textbf{x}) = \textbf{x} - \log \sum_i \exp x_i
                              = \textbf{x} - \log(\sum_i \exp (x_i - m)) - m\]</div>
<p>where <span class="math notranslate nohighlight">\(m\)</span> is the max element of <span class="math notranslate nohighlight">\(\textbf{x}\)</span>. This trick
is common enough that there is a nice derivation on <a class="reference external" href="https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations">wikipedia</a>.
(This is a pratical trick for sigmoid function as well, which we
ignored in earlier modules.)</p>
<p>Speaking of max, we can add a max operator to our code base.  We can
compute the max of a vector (or tensor in general) as a reduction,
which returns the single highest-scored element in the
input. Intuitively, we can think about how small changes to the input
impact this returned value. Ignoring ties, only the element that has
the highest score will have a non-zero derivative, and its derivative
will be 1.  Thereforce the gradient of the max reduction is a one-hot
vector with 1 for the highest-scored element, i.e. the argmax
function.</p>
<p>Here is a summary of how functions for binary classficiaiton connect
with functions for multiclass classification:</p>
<table class="table">
<colgroup>
<col style="width: 37%" />
<col style="width: 63%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Binary</p></th>
<th class="head"><p>Multiclass</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>ReLU</p></td>
<td><p>Max</p></td>
</tr>
<tr class="row-odd"><td><p>Step</p></td>
<td><p>Argmax</p></td>
</tr>
<tr class="row-even"><td><p>Sigmoid</p></td>
<td><p>Softmax</p></td>
</tr>
</tbody>
</table>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="pooling.html" title="previous page">Pooling</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020, Sasha Rush.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>