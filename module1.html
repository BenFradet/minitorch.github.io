
<!DOCTYPE html>

<html lang="english">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Autodiff &#8212; MiniTorch 0.1 documentation</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/css/blank.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/thebelab-helper.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Derivatives" href="derivative.html" />
    <link rel="prev" title="Visualization" href="visualization.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="english">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="index.html">
  <img src="_static/minitorch.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="setup.html">
  Setup
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="mlprimer.html">
  ML Primer
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module0.html">
  Fundamentals
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="current reference internal nav-link" href="#">
  Autodiff
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module2.html">
  Tensors
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module3.html">
  Efficiency
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module4.html">
  Networks
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/minitorch/" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/srush_nlp" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p>
 <span class="caption-text">
  Guides
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="derivative.html">
   Derivatives
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="scalar.html">
   Tracking Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chainrule.html">
   Autodifferentiation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="backpropagate.html">
   Backpropagation
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tasks">
   Tasks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-1-1-numerical-derivatives">
     Task 1.1: Numerical Derivatives
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-1-2-scalars">
     Task 1.2: Scalars
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-1-3-chain-rule">
     Task 1.3: Chain Rule
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-1-4-backpropagation">
     Task 1.4: Backpropagation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-1-5-training">
     Task 1.5: Training
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="autodiff">
<h1>Autodiff<a class="headerlink" href="#autodiff" title="Permalink to this headline">Â¶</a></h1>
<img alt="_images/backprop4.png" class="align-center" src="_images/backprop4.png" />
<p>This module shows how to build the first version of MiniTorch
(mini-MiniTorch?) using only Scalar values. This covers key aspects
of auto-differentiation: the key technique in the system. Then you will
use your code to train a preliminary model.</p>
<p>All starter code is available in <a class="reference external" href="https://github.com/minitorch/Module-1">https://github.com/minitorch/Module-1</a> .</p>
<p>To begin, remember to activate your virtual environment first, and then
clone your assignment:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">git</span> <span class="n">clone</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT1_URL</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cd</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT_NAME</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Ue</span> <span class="o">.</span>
</pre></div>
</div>
<p>Module 1 is built upon the previous Module 0, so make sure to pull your
files from Assignment 0
to your new repo.</p>
<p>Please continue to follow the <a class="reference internal" href="contributing.html"><span class="doc">Contributing</span></a> guideline.</p>
<div class="toctree-wrapper compound">
<p><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="derivative.html">Derivatives</a></li>
<li class="toctree-l1"><a class="reference internal" href="scalar.html">Tracking Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="chainrule.html">Autodifferentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="backpropagate.html">Backpropagation</a></li>
</ul>
</div>
<section id="tasks">
<h2>Tasks<a class="headerlink" href="#tasks" title="Permalink to this headline">Â¶</a></h2>
<section id="task-1-1-numerical-derivatives">
<h3>Task 1.1: Numerical Derivatives<a class="headerlink" href="#task-1-1-numerical-derivatives" title="Permalink to this headline">Â¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with derivatives.
Be sure to review <a class="reference external" href="https://en.wikipedia.org/wiki/Differentiation_rules">differentiation rules</a>
and the notation for derivatives.
Then carefully read the Guide on
<a class="reference internal" href="derivative.html"><span class="doc">Derivatives</span></a>.</p>
</div>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Complete the following function in <cite>minitorch/scalar.py</cite> and pass tests
marked as <cite>task1_1</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.scalar.central_difference">
<code class="sig-prename descclassname">minitorch.scalar.</code><code class="sig-name descname">central_difference</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">f</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">vals</span></em>, <em class="sig-param"><span class="n">arg</span><span class="o">=</span><span class="default_value">0</span></em>, <em class="sig-param"><span class="n">epsilon</span><span class="o">=</span><span class="default_value">1e-06</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.central_difference" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Computes an approximation to the derivative of <cite>f</cite> with respect to one arg.</p>
<p>See <a class="reference internal" href="derivative.html"><span class="doc">Derivatives</span></a> or <a class="reference external" href="https://en.wikipedia.org/wiki/Finite_difference">https://en.wikipedia.org/wiki/Finite_difference</a> for more details.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>f</strong> -- arbitrary function from n-scalar args to one value</p></li>
<li><p><strong>*vals</strong> (<em>list of floats</em>) -- n-float values <span class="math notranslate nohighlight">\(x_0 \ldots x_{n-1}\)</span></p></li>
<li><p><strong>arg</strong> (<em>int</em>) -- the number <span class="math notranslate nohighlight">\(i\)</span> of the arg to compute the derivative</p></li>
<li><p><strong>epsilon</strong> (<em>float</em>) -- a small constant</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>An approximation of <span class="math notranslate nohighlight">\(f'_i(x_0, \ldots, x_{n-1})\)</span></p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>float</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="task-1-2-scalars">
<h3>Task 1.2: Scalars<a class="headerlink" href="#task-1-2-scalars" title="Permalink to this headline">Â¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with the <a class="reference internal" href="scalar.html#minitorch.Scalar" title="minitorch.Scalar"><code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Scalar</span></code></a> class.
Be sure to first carefully read the Guide on
<a class="reference internal" href="scalar.html"><span class="doc">Tracking Variables</span></a> and to refresh your memory on <a class="reference external" href="https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types/">Python numerical overrides</a>.</p>
</div>
<p>Implement the overridden mathematical functions required for the
<a class="reference internal" href="scalar.html#minitorch.Scalar" title="minitorch.Scalar"><code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Scalar</span></code></a> class.
Each of these requires wiring the internal Python operator to the correct
<code class="xref py py-func docutils literal notranslate"><span class="pre">minitorch.Function.forward()</span></code> call.</p>
<dl class="py function">
<dt id="minitorch.scalar.ScalarFunction.forward">
<code class="sig-prename descclassname">minitorch.scalar.ScalarFunction.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">inputs</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.ScalarFunction.forward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Forward call, compute <span class="math notranslate nohighlight">\(f(x_0 \ldots x_{n-1})\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Context</span></code>) -- A container object to save
any information that may be needed
for the call to backward.</p></li>
<li><p><strong>*inputs</strong> (<em>list of floats</em>) -- n-float values <span class="math notranslate nohighlight">\(x_0 \ldots x_{n-1}\)</span>.</p></li>
</ul>
</dd>
</dl>
<p>Should return float the computation of the function <span class="math notranslate nohighlight">\(f\)</span>.</p>
</dd></dl>

<p>Read the example ScalarFunctions that we have implemented for guidelines. You
may find it useful to reuse the operators from Module 0.</p>
<p>We have built a debugging tool for you to observe the workings of your
expressions to see how the graph is built. You can run it in the
<cite>Autodiff Sandbox</cite>.  You can alter the expression at the top of the
file and then run the code to create a graph in <cite>Streamlit</cite>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">streamlit</span> <span class="n">run</span> <span class="n">app</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span> <span class="mi">1</span>
</pre></div>
</div>
<div class="admonition-todo admonition" id="id2">
<p class="admonition-title">Todo</p>
<p>Complete the following functions in <cite>minitorch/scalar.py</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.scalar.Mul.forward">
<code class="sig-prename descclassname">minitorch.scalar.Mul.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Mul.forward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Inv.forward">
<code class="sig-prename descclassname">minitorch.scalar.Inv.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Inv.forward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Neg.forward">
<code class="sig-prename descclassname">minitorch.scalar.Neg.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Neg.forward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Sigmoid.forward">
<code class="sig-prename descclassname">minitorch.scalar.Sigmoid.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Sigmoid.forward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.ReLU.forward">
<code class="sig-prename descclassname">minitorch.scalar.ReLU.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.ReLU.forward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Exp.forward">
<code class="sig-prename descclassname">minitorch.scalar.Exp.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Exp.forward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.LT.forward">
<code class="sig-prename descclassname">minitorch.scalar.LT.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.LT.forward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.EQ.forward">
<code class="sig-prename descclassname">minitorch.scalar.EQ.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.EQ.forward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<div class="admonition-todo admonition" id="id4">
<p class="admonition-title">Todo</p>
<p>Complete the following function in <cite>minitorch/scalar.py</cite>, and pass
tests marked as <cite>task1_2</cite>.
See <a class="reference external" href="https://docs.python.org/3/reference/datamodel.html#emulating-numeric-types/">Python numerical overrides</a>
for the interface of these methods. All of these functions should return
<a class="reference internal" href="scalar.html#minitorch.Scalar" title="minitorch.Scalar"><code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Scalar</span></code></a> arguments.</p>
</div>
<dl class="py function">
<dt id="minitorch.Scalar.__lt__">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">__lt__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.__lt__" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.__gt__">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">__gt__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.__gt__" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.__sub__">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">__sub__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.__sub__" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.__neg__">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">__neg__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.__neg__" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.__add__">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">__add__</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.__add__" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.log">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">log</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.log" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.exp">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">exp</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.exp" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.sigmoid">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">sigmoid</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.sigmoid" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.Scalar.relu">
<code class="sig-prename descclassname">minitorch.Scalar.</code><code class="sig-name descname">relu</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.Scalar.relu" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</section>
<section id="task-1-3-chain-rule">
<h3>Task 1.3: Chain Rule<a class="headerlink" href="#task-1-3-chain-rule" title="Permalink to this headline">Â¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task is quite tricky, so be sure you
understand the chain rule, Variables, and Functions.
Be sure to first read the Guide on
<a class="reference internal" href="chainrule.html"><span class="doc">Autodifferentiation</span></a> very carefully and read the code for other ScalarFunctions.</p>
</div>
<p>Implement the <cite>chain_rule</cite> function on FunctionBase for functions of arbitrary
arguments.
This function should be able to backward process a function by passing it in
a context and <span class="math notranslate nohighlight">\(d_{out}\)</span> and then collecting the local derivatives. It
should then pair these with the right variables and return them. This function
is also where we filter out constants that were used on the forward pass,
but do not need derivatives.</p>
<div class="admonition-todo admonition" id="id5">
<p class="admonition-title">Todo</p>
<p>Complete the following function in <cite>minitorch/autodiff.py</cite>, and pass
tests marked as <cite>task1_3</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.FunctionBase.chain_rule">
<code class="sig-prename descclassname">minitorch.FunctionBase.</code><code class="sig-name descname">chain_rule</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">inputs</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.FunctionBase.chain_rule" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Implement the derivative chain-rule.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Context</span></code>) -- The context from running forward</p></li>
<li><p><strong>inputs</strong> (<em>list of args</em>) -- The args that were passed to <code class="xref py py-func docutils literal notranslate"><span class="pre">FunctionBase.apply()</span></code> (e.g. <span class="math notranslate nohighlight">\(x, y\)</span>)</p></li>
<li><p><strong>d_output</strong> (<em>number</em>) -- The <cite>d_output</cite> value in the chain rule.</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>A list of non-constant variables with their derivatives
(see <cite>is_constant</cite> to remove unneeded variables)</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of (<cite>Variable</cite>, number)</p>
</dd>
</dl>
</dd></dl>

</section>
<section id="task-1-4-backpropagation">
<h3>Task 1.4: Backpropagation<a class="headerlink" href="#task-1-4-backpropagation" title="Permalink to this headline">Â¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Be sure to first read the Guide on
<a class="reference internal" href="backpropagate.html"><span class="doc">Backpropagation</span></a> very carefully and read the code for other
ScalarFunctions.</p>
</div>
<p>Implement backpropagation. Each of these requires wiring the internal Python
operator to the correct
<code class="xref py py-func docutils literal notranslate"><span class="pre">minitorch.Function.backward()</span></code> call.</p>
<dl class="py function">
<dt id="minitorch.scalar.ScalarFunction.backward">
<code class="sig-prename descclassname">minitorch.scalar.ScalarFunction.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_out</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.ScalarFunction.backward" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Backward call, computes <span class="math notranslate nohighlight">\(f'_{x_i}(x_0 \ldots x_{n-1}) \times d_{out}\)</span>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>ctx</strong> (<em>Context</em>) -- A container object holding any information saved during in the corresponding <cite>forward</cite> call.</p></li>
<li><p><strong>d_out</strong> (<em>float</em>) -- <span class="math notranslate nohighlight">\(d_out\)</span> term in the chain rule.</p></li>
</ul>
</dd>
</dl>
<p>Should return the computation of the derivative function
<span class="math notranslate nohighlight">\(f'_{x_i}\)</span> for each input <span class="math notranslate nohighlight">\(x_i\)</span> times <cite>d_out</cite>.</p>
</dd></dl>

<p>Read the example ScalarFunctions that we have implemented for
guidelines. Feel free to also consult <a class="reference external" href="https://en.wikipedia.org/wiki/Differentiation_rules">differentiation rules</a> if you forget how
these identities work.</p>
<div class="admonition-todo admonition" id="id7">
<p class="admonition-title">Todo</p>
<p>Complete the following functions in <cite>minitorch/autodiff.py</cite> and
<cite>minitorch/scalar.py</cite>,
and pass tests marked as <cite>task1_4</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.topological_sort">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">topological_sort</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">variable</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.topological_sort" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Computes the topological order of the computation graph.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>variable</strong> (<a class="reference internal" href="scalar.html#minitorch.Variable" title="minitorch.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a>) -- The right-most variable</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p><dl class="simple">
<dt>Non-constant Variables in topological order</dt><dd><p>starting from the right.</p>
</dd>
</dl>
</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list of Variables</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.backpropagate">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">backpropagate</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">variable</span></em>, <em class="sig-param"><span class="n">deriv</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.backpropagate" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Runs backpropagation on the computation graph in order to
compute derivatives for the leave nodes.</p>
<p>See <a class="reference internal" href="backpropagate.html"><span class="doc">Backpropagation</span></a> for details on the algorithm.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>variable</strong> (<a class="reference internal" href="scalar.html#minitorch.Variable" title="minitorch.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">Variable</span></code></a>) -- The right-most variable</p></li>
<li><p><strong>deriv</strong> (<em>number</em>) -- Its derivative that we want to propagate backward to the leaves.</p></li>
</ul>
</dd>
</dl>
<p>No return. Should write to its results to the derivative values of each leaf through <cite>accumulate_derivative</cite>.</p>
</dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Mul.backward">
<code class="sig-prename descclassname">minitorch.scalar.Mul.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Mul.backward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Inv.backward">
<code class="sig-prename descclassname">minitorch.scalar.Inv.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Inv.backward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Neg.backward">
<code class="sig-prename descclassname">minitorch.scalar.Neg.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Neg.backward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Sigmoid.backward">
<code class="sig-prename descclassname">minitorch.scalar.Sigmoid.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Sigmoid.backward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.ReLU.backward">
<code class="sig-prename descclassname">minitorch.scalar.ReLU.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.ReLU.backward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.scalar.Exp.backward">
<code class="sig-prename descclassname">minitorch.scalar.Exp.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">d_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.scalar.Exp.backward" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

</section>
<section id="task-1-5-training">
<h3>Task 1.5: Training<a class="headerlink" href="#task-1-5-training" title="Permalink to this headline">Â¶</a></h3>
<p>If your code works, you should now be able to run the training script.
Study the code in <cite>project/run_scalar.py</cite> carefully to understand what
the neural network is doing.</p>
<p>You will also need Module code to implement the parameters <cite>Network</cite>
and for <cite>Linear</cite>.
You can modify the dataset and the module with
the parameters at the bottom of the file. Start with this simple config:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">PTS</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">DATASET</span> <span class="o">=</span> <span class="n">minitorch</span><span class="o">.</span><span class="n">datasets</span><span class="p">[</span><span class="s2">&quot;Simple&quot;</span><span class="p">](</span><span class="n">PTS</span><span class="p">)</span>
<span class="n">HIDDEN</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">RATE</span> <span class="o">=</span> <span class="mf">0.5</span>
</pre></div>
</div>
<p>You can then move up to something more complex, for instance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">PTS</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">DATASET</span> <span class="o">=</span> <span class="n">minitorch</span><span class="o">.</span><span class="n">datasets</span><span class="p">[</span><span class="s2">&quot;Xor&quot;</span><span class="p">](</span><span class="n">PTS</span><span class="p">)</span>

<span class="n">HIDDEN</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">RATE</span> <span class="o">=</span> <span class="mf">0.5</span>
</pre></div>
</div>
<p>If your code is successful, you should be able to run the full visualization:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">streamlit</span> <span class="n">run</span> <span class="n">app</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span> <span class="mi">1</span>
</pre></div>
</div>
<div class="admonition-todo admonition" id="id8">
<p class="admonition-title">Todo</p>
<p>Train a scalar model for each of the 4 main datasets.</p>
<p>Add the output training logs and final images to your README file.</p>
</div>
</section>
</section>
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="visualization.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Visualization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="derivative.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Derivatives</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020, Sasha Rush.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>