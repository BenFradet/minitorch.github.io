Assignment 2 - Tensors
********************************

In the last section we developed a full autodifferentiation system
using scalars. This system is fully correct, but as we saw, it is not
very efficient. Each operation required storing a graph of all the
values that we had previously created; training required repeating
these operations; computing modules such as linear required a `for`
loop over each of the terms in our network. In this homework we
introduce and implement as  new `tensor` object that will allow us to
do all of these operations in a single steps.



*Before starting any of this assignment*, Be sure to carefully read
the :doc:`tensor.rst` and :doc:`contributing.rst`. 


=============
Task 0: Setup
=============

To begin, remember to first activate your virtual environment.

>>> source venv/bin/activate

And then clone your assignment.

>>> git clone {{STUDENT_ASSIGNMENT2_URL}}
>>> cd minitorch2

You will need the files from Assignment 0 so be sure to pull them over
to your new repo.

  
==========================================
Tasks 1: Tensor Data - Indexing
==========================================


* implement `count`
* implement `size`
* implement `index`
* implment `enumerate`


==========================================
Tasks 2: Tensor Operations
==========================================

Now that we have a tensor class, our plan will be to reimplement all
our mathematical operations on top of this framework. Instead of
adding scalars we will add tensors. The goal is to make this feel
simple and intuitive to users of the library. ::


  # a) Return a new tensor with the same shape as `tensor_a` where
  # each position is the log/exp/negative of the position in `tensor_a`
  tensor_a.log()
  tensor_a.exp()
  -tensor_a
  ...

  # b) Return a new tensor where
  # each position is the sum/mul/sub of the position in `tensor_a` and `tensor_b`
  tensor_a + tensor_b
  tensor_a * tensor_b
  tensor_a - tensor_b
  ...

  # c) Return a new tensor where dim-1 is size 1 and represents
  # the sum/mean over dim-1 in `tensor_a`
  tensor_a.sum(1)
  tensor_a.mean(1)
  ...

However, instead of directly implementing each of these operations
individually, let's be a bit lazy and note the structural
similarities. We can then implement helpers that do most of the work
for us.

Operation a / map: These operations all just touch each of the
positions of the tensor individually. They don't need to deal with
other positions or know anything about the shape or size of the tensor.

.. image:: bigger.png

Operation b / zip: These operations need to pair operations between
neighboring tensors.


.. image:: bigger.png


Operation c / reduce: These operations need to group together cells within a single tensor.

.. image:: bigger.png


* implement `map`
* implement `zip`
* implement `reduce`


The tensor operations from the last section allow you to map, zip, and
reduce tensor data objects together. On top of this foundation we can
build up a similar class to the Scalar class in
assignment 1. We will call this object `Tensor` to distinguish it from
the raw tensor data.

* read the implementatino for neg, add, mul
* implement the other forward operations for tensor. (pass tests)


==========================================
Tasks 3: Gradients and Autograd
==========================================

We have now moved from scalars and derivatives to vector, matrices and
tensors.  In theory, this means `multivariate calculus` and somewhat
scary terminology. However, most of what we actually need to do will
not require complicated terminology or much technical math. This
section will give a primer to the main ideas, building on assignment 1.

The main new term you need to know is `gradient`. A tensor is a
multidimensional array of scalars, a gradient is a multidimensional
array of derivatives for these scalars. ::


  # Assignment 1 notation
  out = f(a, b, c)
  out.backward()
  (a.derivative, b.derivative, c.derivative)


  # Assignment 2 notation
  tensor1 = tensor(a, b, c)
  out = g(tensor1)
  out.backward()

  # shape (3,)
  tensor1.grad

The gradient of `tensor1` is a tensor that holds the derivatives of
each of its values.


Now, if you look online, you will find lots of different notation for
gradients and multivariate terminology. For this assignment, I ask
that you ignore it and stick to everything you know about derivatives.
It turns out that you can do must of machine learning without ever
thinking in higher dimensions.

How could this be true? Well lets look at the operations from the
previous section.

1) `map`. Given a tensor, map applies a fixed operation to each scalar
   position individually. For a scalar :math:`x`, it computes
   :math:`g(x)`.  Therefore, from assignment 1, we know that the
   derivative :math:`f(g(x))` is equal to :math:`g'(x) \times d`. This
   means to compute the gradient, we only need to compute the
   derivative for each position and map.


.. image:: bigger.png

2) `zip`. Given two tensors, zip applies a fixed operation to each
   scalar position individually. For two scalars :math:`x` and
   :math:`y`, it computes :math:`g(x, y)`.  Therefore, from assignment
   1, we know that the derivative :math:`f(g(x, y))` is equal to
   :math:`g_x'(x, y) \times d` and :math:`g_y'(x, y) \times d`. This
   means to compute the gradient, we only need to compute the
   derivative for each position and map.

.. image:: bigger.png

3) `reduce`. Given a tensor, reduce applies a fixed aggregation
   operation to one dimension. For simplicity lets consider sum-based
   reductions.  For scalars :math:`x_1` to :math:`x_n`, it computes
   :math:`x_1 + x_2 + \ldots`.  For any :math:`x` value this
   yields 1. Therefore, the derivative for any position is simply the
   derivative passed backward :math:`d`. This means to compute the
   gradient, we only need to expand the derivative for each
   position. For other reduce operations such as `product`, you get
   different expansions (but these can be calculated just by taking
   derivatives).

.. image:: bigger.png


* read the implementatino for neg, add, mul
* implement the other forward operations for tensor. (pass tests)



==========================================
Tasks 4: Tensor Broadcasting
==========================================

So far all of our `zip` operations have assumed that we had two
tensors of exactly the same size and shape. However there are many
times when it is interesting to `zip` two tensors are different size.


Perhaps the simplest case is when we have one vector (1-D tensor) and want to add a
constant to every position ::

  vector1 + tensor([10])

We would like for this operation to have the standard interpretation
of adding 10 to each position.  We can view the constant as a tensor of
size 1 and shape (1,) and assume `vector1` is of shape (3,).
Because of this, simple `zip` will fail.

A natural fix is to `broadcast` this dimension to automatically grow
to the size of the vector. That is inside zip we "pretend" that 10 is a vector of
size (3,) that repeats its value, i.e. ::

  vector1 + tensor([10, 10, 10])


(Note we never actually create this tensor. This is just an interpretation.)


* Rule 1: Any dimension of size 1, can be zipped with dimensions of size n > 1 by assuming the dimension is copied n times.


Now let's try apply this approach to a matrix of shape (4, 3). ::

    matrix1 + tensor([10])


Here we are trying to zip a matrix (2-D) of shape (4, 3) with a vector
(1-D) of shape (1,).  It seems like this should fail, but remember we
are always allowed to add "empty" (shape-1) dimensions. If we add an
empty dimension and then apply rule 1 twice we can zip the two together. ::

    matrix1 + tensor([10] * 12, shape=(4, 3))

(Note we never actually create this tensor. This is just an interpretation.)


* Rule 2: Any extra dimensions of size 1 can be added to a tensor to ensure that the sizes are the same.


Finally there is a question of where to add the empty dimensions. This
is not an issue in the above case but becomes an issue in more
complicated cases. ::

  # These two lines are equivalent
  matrix1 + vector1
  matrix1 + vector1.view(1, 3)


* Rule 3: Any extra dimensions of size 1 are always added on the left-side of the shape.

This rule has the impact of making the process easy to follow and replicate. You always know what the shape of the final
output will be. For instance, ::


  # This will fail!
  matrix1.permute(1, 0) + vector1


What's happenining here? The issue is that after the `permute` the shape (3, 4) and we are adding a vector of size
(3,). It seems like this should be okay, but the remember rule 3, we can only add new dimensions to the left. If we want to add
right dimensions we need to do it manually.::

  # This will work!
  matrix1.permute(1, 0) + vector1.view(3, 1)


Finally we note that we can use the broadcasting rules and many times as we want in a given setting:  ::

  # This will return shape (7, 2, 3, 5)
  tensor1.view(2, 3, 1) + tensor2.view(7, 2, 1, 5)

* implement `get`
* implement `shape_broadcast`



==========================================
Tasks 5: Training
==========================================


* Read project `tensor`
