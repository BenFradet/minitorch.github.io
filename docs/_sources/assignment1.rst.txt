Assignment 1 - Auto-Derivatives
********************************


Now that preliminaries are done we can really get started.  In this
assignment you will implement a complete basic version of MiniTorch
using scalar values. You will then use this system to train a
mini-model and get your first real results.


=============
Task 0: Setup
=============

To begin, remember to first activate your virtual environment.

>>> source venv/bin/activate

And then clone your assignment.

>>> git clone {{STUDENT_ASSIGNMENT1_URL}}
>>> cd minitorch1

You will need the files from Assignment 0 so be sure to pull them over
to your new repo.




===============================
Task 1: Approximate Derivatives
================================


Let us begin by discussing derivatives in the setting of
programming. If we are given a function,

.. math ::

   f(x) = sin(2 x)

We can compute a function for its derivative applying rules from univariate calculus, i.e.

.. math ::

    f'(x) = 2 \times cos(2 x)

We will refer to this as the symbolic derivative of the function. In
some sense this is the ideal term to compute since it tells use
everything we need to know about the derivative of the function.



Practically, this means that there is another way to compute approximate derivative that works for black-box functions. You simply pick as value of epsilon, say 1e-6, and compute the above term. This leads to a `higher-order function` of the following form ::

    def approx_derivative(f, x):
        ...



* Implement this function.

*

========================
Task 2: Scalars
========================


Symbolic derivatives require knowing the whole function, whereas our
above approximation requires only a single black box. We hope to build
something in between, `autoderivatives`. However to do this we need to
collect more information about the function f(x) itself. This can be
hard to do since Python does not expose how x is used in the function
directly, all we get is the output.

.. autoclass:: minitorch.Scalar


* read python operator overloading. Add calls in scalar.py for
    def __mul__(self, b):
    def __radd__(self, b):
    def __add__(self, b):
    def __rmul__(self, b):
    def __lt__(self, b):
    def __gt__(self, b):
    def __sub__(self, b):
    def __neg__(self):


Once we have this overloading we need to define each of the functions that we use in a special
way.



* Implement Scalar Functions for each of the terms.

* Pass the tests in `test_scalar`.


========================
Task 3: Auto-Derivative
========================

Now we have a cute way of grouping together function implementation
and implmentations of their symbolic derivatives. We are making
progress, but we still do not have a better method for taking
derivatives of unseen functions. This is where `autodifferentiation`
comes in. It provides a way to chain together individual scalar
functions.



We can also view this as a graph.

.. image ::


* Every `Scalar` stores what was the last function that computed it.

.. autofunction:: minitorch.FunctionBase.chain_rule
.. autofunction:: minitorch.backpropagate

                  
  
========================
Task 4: Training
========================
