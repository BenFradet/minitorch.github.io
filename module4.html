
<!DOCTYPE html>

<html lang="english">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Networks &#8212; MiniTorch 0.1 documentation</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/thebelab-helper.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Convolution" href="convolution.html" />
    <link rel="prev" title="GPU Programming" href="cuda.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="index.html">
  <img src="_static/minitorch.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="setup.html">
  Setup
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="mlprimer.html">
  ML Primer
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module0.html">
  Fundamentals
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module1.html">
  Autodiff
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module2.html">
  Tensors
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module3.html">
  Efficiency
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="current reference internal nav-link" href="#">
  Networks
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/minitorch/" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/srush_nlp" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption">
 <span class="caption-text">
  Guides
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="convolution.html">
   Convolution
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="pooling.html">
   Pooling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="softmax.html">
   Softmax
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tasks">
   Tasks
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-4-1-1d-convolution">
     Task 4.1: 1D Convolution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-4-2-2d-convolution">
     Task 4.2: 2D Convolution
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-4-3-pooling">
     Task 4.3: Pooling
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-4-4-softmax-and-dropout">
     Task 4.4: Softmax and Dropout
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-4-4b-extra-credit">
     Task 4.4b: Extra Credit
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#task-4-5-training-an-image-classifier">
     Task 4.5: Training an Image Classifier
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="networks">
<h1>Networks<a class="headerlink" href="#networks" title="Permalink to this headline">Â¶</a></h1>
<a class="reference internal image-reference" href="_images/orig.png"><img alt="_images/orig.png" class="align-center" src="_images/orig.png" style="width: 400px;" /></a>
<a class="reference internal image-reference" href="_images/mnist2.png"><img alt="_images/mnist2.png" class="align-center" src="_images/mnist2.png" style="width: 400px;" /></a>
<a class="reference internal image-reference" href="_images/mnist5.png"><img alt="_images/mnist5.png" class="align-center" src="_images/mnist5.png" style="width: 400px;" /></a>
<a class="reference internal image-reference" href="_images/mnist4.png"><img alt="_images/mnist4.png" class="align-center" src="_images/mnist4.png" style="width: 400px;" /></a>
<p>We now have a fully working deep learning library with most of the
features of a real industrial system like Torch. To take advantage of
this hard work, this module is entirely based on using the
software framework. In particular, we are going to build an image
recognition system. We will do this by build the infrastructure for a
version of LeNet on MNIST: a classic convolutional neural network (CNN)
for digit recognition.</p>
<img alt="_images/networkcnn.png" class="align-center" src="_images/networkcnn.png" />
<p>All starter code is available in <a class="reference external" href="https://github.com/minitorch/Module-4">https://github.com/minitorch/Module-4</a> .</p>
<p>To begin, remember to activate your virtual environment first, and
then clone your assignment:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">git</span> <span class="n">clone</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT4_URL</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cd</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT_NAME</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">Ue</span> <span class="o">.</span>
</pre></div>
</div>
<p>You need the files from previous assignments, so maker sure to pull
them over to your new repo.  We recommend you to get familiar with
tensor.py, since you might find some of those functions useful for
implementing this Module.</p>
<p>Additionally, you need to install and download an MNIST library:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">pip</span> <span class="n">install</span> <span class="n">python</span><span class="o">-</span><span class="n">mnist</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mnist_get_data</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>(Mac OS users may need to install wget in order to run the .sh file.)</p>
<p>It will add a <cite>data/</cite> directory in your module.  You can try the
following code to test the installation:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mnist</span> <span class="kn">import</span> <span class="n">MNIST</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="n">mndata</span> <span class="o">=</span> <span class="n">MNIST</span><span class="p">(</span><span class="s2">&quot;data/&quot;</span><span class="p">)</span>
<span class="n">images</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">mndata</span><span class="o">.</span><span class="n">load_training</span><span class="p">()</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">images</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">im</span> <span class="o">=</span> <span class="n">im</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">im</span><span class="p">)</span>
</pre></div>
</div>
<p>Be sure to continue to follow the <a class="reference internal" href="contributing.html"><span class="doc">Contributing</span></a> guidelines.</p>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="convolution.html">Convolution</a></li>
<li class="toctree-l1"><a class="reference internal" href="pooling.html">Pooling</a></li>
<li class="toctree-l1"><a class="reference internal" href="softmax.html">Softmax</a></li>
</ul>
</div>
<div class="section" id="tasks">
<h2>Tasks<a class="headerlink" href="#tasks" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="task-4-1-1d-convolution">
<h3>Task 4.1: 1D Convolution<a class="headerlink" href="#task-4-1-1d-convolution" title="Permalink to this headline">Â¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with convolution.
Be sure to read the Guide on <a class="reference internal" href="convolution.html"><span class="doc">Convolution</span></a>.</p>
</div>
<p>You will implement the 1D convolution in Numba. This function gets used by the
<cite>forward</cite> and <cite>backward</cite> pass of conv1d.</p>
<a class="reference internal image-reference" href="_images/channels.png"><img alt="_images/channels.png" class="align-center" src="_images/channels.png" style="width: 300px;" /></a>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Complete the following function in <cite>minitorch/fast_conv.py</cite>, and pass tests marked
as <cite>task4_1</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.tensor_conv1d">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">tensor_conv1d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">out</span></em>, <em class="sig-param"><span class="n">out_shape</span></em>, <em class="sig-param"><span class="n">out_strides</span></em>, <em class="sig-param"><span class="n">out_size</span></em>, <em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">input_shape</span></em>, <em class="sig-param"><span class="n">input_strides</span></em>, <em class="sig-param"><span class="n">weight</span></em>, <em class="sig-param"><span class="n">weight_shape</span></em>, <em class="sig-param"><span class="n">weight_strides</span></em>, <em class="sig-param"><span class="n">reverse</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor_conv1d" title="Permalink to this definition">Â¶</a></dt>
<dd><p>1D Convolution implementation.</p>
<p>Given input tensor of</p>
<blockquote>
<div><p><cite>batch, in_channels, width</cite></p>
</div></blockquote>
<p>and weight tensor</p>
<blockquote>
<div><p><cite>out_channels, in_channels, k_width</cite></p>
</div></blockquote>
<p>Computes padded output of</p>
<blockquote>
<div><p><cite>batch, out_channels, width</cite></p>
</div></blockquote>
<p><cite>reverse</cite> decides if weight is anchored left (False) or right.
(See diagrams)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_size</strong> (<em>int</em>) -- size of the <cite>out</cite> tensor.</p></li>
<li><p><strong>input</strong> (<em>array</em>) -- storage for <cite>input</cite> tensor.</p></li>
<li><p><strong>input_shape</strong> (<em>array</em>) -- shape for <cite>input</cite> tensor.</p></li>
<li><p><strong>input_strides</strong> (<em>array</em>) -- strides for <cite>input</cite> tensor.</p></li>
<li><p><strong>weight</strong> (<em>array</em>) -- storage for <cite>input</cite> tensor.</p></li>
<li><p><strong>weight_shape</strong> (<em>array</em>) -- shape for <cite>input</cite> tensor.</p></li>
<li><p><strong>weight_strides</strong> (<em>array</em>) -- strides for <cite>input</cite> tensor.</p></li>
<li><p><strong>reverse</strong> (<em>bool</em>) -- anchor weight at left or right</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="task-4-2-2d-convolution">
<h3>Task 4.2: 2D Convolution<a class="headerlink" href="#task-4-2-2d-convolution" title="Permalink to this headline">Â¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with convolution.
Be sure to read the Guide on <a class="reference internal" href="convolution.html"><span class="doc">Convolution</span></a>.</p>
</div>
<p>You will implement the 2D convolution in Numba. This function gets used by the
<cite>forward</cite> and <cite>backward</cite> pass of conv2d.</p>
<a class="reference internal image-reference" href="_images/conv2.png"><img alt="_images/conv2.png" class="align-center" src="_images/conv2.png" style="width: 400px;" /></a>
<div class="admonition-todo admonition" id="id2">
<p class="admonition-title">Todo</p>
<p>Complete the following function in <cite>minitorch/fast_conv.py</cite>, and pass tests marked
as <cite>task4_2</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.tensor_conv2d">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">tensor_conv2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">out</span></em>, <em class="sig-param"><span class="n">out_shape</span></em>, <em class="sig-param"><span class="n">out_strides</span></em>, <em class="sig-param"><span class="n">out_size</span></em>, <em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">input_shape</span></em>, <em class="sig-param"><span class="n">input_strides</span></em>, <em class="sig-param"><span class="n">weight</span></em>, <em class="sig-param"><span class="n">weight_shape</span></em>, <em class="sig-param"><span class="n">weight_strides</span></em>, <em class="sig-param"><span class="n">reverse</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor_conv2d" title="Permalink to this definition">Â¶</a></dt>
<dd><p>2D Convolution implementation.</p>
<p>Given input tensor of</p>
<blockquote>
<div><p><cite>batch, in_channels, height, width</cite></p>
</div></blockquote>
<p>and weight tensor</p>
<blockquote>
<div><p><cite>out_channels, in_channels, k_height, k_width</cite></p>
</div></blockquote>
<p>Computes padded output of</p>
<blockquote>
<div><p><cite>batch, out_channels, height, width</cite></p>
</div></blockquote>
<p><cite>Reverse</cite> decides if weight is anchored top-left (False) or bottom-right.
(See diagrams)</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_size</strong> (<em>int</em>) -- size of the <cite>out</cite> tensor.</p></li>
<li><p><strong>input</strong> (<em>array</em>) -- storage for <cite>input</cite> tensor.</p></li>
<li><p><strong>input_shape</strong> (<em>array</em>) -- shape for <cite>input</cite> tensor.</p></li>
<li><p><strong>input_strides</strong> (<em>array</em>) -- strides for <cite>input</cite> tensor.</p></li>
<li><p><strong>weight</strong> (<em>array</em>) -- storage for <cite>input</cite> tensor.</p></li>
<li><p><strong>weight_shape</strong> (<em>array</em>) -- shape for <cite>input</cite> tensor.</p></li>
<li><p><strong>weight_strides</strong> (<em>array</em>) -- strides for <cite>input</cite> tensor.</p></li>
<li><p><strong>reverse</strong> (<em>bool</em>) -- anchor weight at top-left or bottom-right</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="task-4-3-pooling">
<h3>Task 4.3: Pooling<a class="headerlink" href="#task-4-3-pooling" title="Permalink to this headline">Â¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with pooling.
Be sure to read the Guide on <a class="reference internal" href="pooling.html"><span class="doc">Pooling</span></a>.</p>
</div>
<p>You will implement 2D pooling on tensors with an average operation.</p>
<a class="reference internal image-reference" href="_images/pool2d.png"><img alt="_images/pool2d.png" class="align-center" src="_images/pool2d.png" style="width: 500px;" /></a>
<div class="admonition-todo admonition" id="id3">
<p class="admonition-title">Todo</p>
<p>Complete the following functions in <cite>minitorch/nn.py</cite>, and pass tests
marked as <cite>task4_3</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.tile">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">tile</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">kernel</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tile" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Reshape an image tensor for 2D pooling</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) -- batch x channel x height x width</p></li>
<li><p><strong>kernel</strong> (<em>pair of ints</em>) -- height x width of pooling</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>Tensor of size batch x channel x new_height x new_width x (kernel_height * kernel_width) as well as the new_height and new_width value.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>(<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, int, int)</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.avgpool2d">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">avgpool2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">kernel</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.avgpool2d" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Tiled average pooling 2D</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) -- batch x channel x height x width</p></li>
<li><p><strong>kernel</strong> (<em>pair of ints</em>) -- height x width of pooling</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>pooled tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="task-4-4-softmax-and-dropout">
<h3>Task 4.4: Softmax and Dropout<a class="headerlink" href="#task-4-4-softmax-and-dropout" title="Permalink to this headline">Â¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires basic familiarity with max reductions.
Be sure to read the Guide on <a class="reference internal" href="softmax.html"><span class="doc">Softmax</span></a>.</p>
</div>
<p>You will implement max, softmax, and log softmax on tensors as well as
the dropout and max-pooling operations.</p>
<a class="reference internal image-reference" href="_images/value.png"><img alt="_images/value.png" class="align-center" src="_images/value.png" style="width: 400px;" /></a>
<a class="reference internal image-reference" href="_images/softmax.png"><img alt="_images/softmax.png" class="align-center" src="_images/softmax.png" style="width: 400px;" /></a>
<div class="admonition-todo admonition" id="id4">
<p class="admonition-title">Todo</p>
<ul class="simple">
<li><p>Complete the following functions in <cite>minitorch/nn.py</cite>, and pass
tests marked as <cite>task4_4</cite>.</p></li>
<li><p>Add a property tests for the function in
<cite>test/test_nn.py</cite> and ensure that you understand its gradient
computation.</p></li>
</ul>
</div>
<dl class="py function">
<dt id="minitorch.max">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">max</code><span class="sig-paren">(</span><em class="sig-param"><span class="o">*</span><span class="n">vals</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.max" title="Permalink to this definition">Â¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.softmax">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">softmax</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">dim</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.softmax" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Compute the softmax as a tensor.</p>
<div class="math notranslate nohighlight">
\[z_i = \frac{e^{x_i}}{\sum_i e^{x_i}}\]</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) -- input tensor</p></li>
<li><p><strong>dim</strong> (<em>int</em>) -- dimension to apply softmax</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>softmax tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.logsoftmax">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">logsoftmax</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">dim</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.logsoftmax" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Compute the log of the softmax as a tensor.</p>
<div class="math notranslate nohighlight">
\[z_i = x_i - \log \sum_i e^{x_i}\]</div>
<p>See <a class="reference external" href="https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations">https://en.wikipedia.org/wiki/LogSumExp#log-sum-exp_trick_for_log-domain_calculations</a></p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) -- input tensor</p></li>
<li><p><strong>dim</strong> (<em>int</em>) -- dimension to apply log-softmax</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>log of softmax tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.maxpool2d">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">maxpool2d</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">kernel</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.maxpool2d" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Tiled max pooling 2D</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) -- batch x channel x height x width</p></li>
<li><p><strong>kernel</strong> (<em>pair of ints</em>) -- height x width of pooling</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>pooled tensor</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.dropout">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">dropout</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">input</span></em>, <em class="sig-param"><span class="n">rate</span></em>, <em class="sig-param"><span class="n">ignore</span><span class="o">=</span><span class="default_value">False</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.dropout" title="Permalink to this definition">Â¶</a></dt>
<dd><p>Dropout positions based on random noise.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>input</strong> (<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>) -- input tensor</p></li>
<li><p><strong>rate</strong> (<em>float</em>) -- probability [0, 1) of dropping out each position</p></li>
<li><p><strong>ignore</strong> (<em>bool</em>) -- skip dropout, i.e. do nothing at all</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>tensor with randoom positions dropped out</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code></p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="task-4-4b-extra-credit">
<h3>Task 4.4b: Extra Credit<a class="headerlink" href="#task-4-4b-extra-credit" title="Permalink to this headline">Â¶</a></h3>
<p>Implementing convolution and pooling efficiently is critical for
large-scale image recognition. However, both are a bit harder than
some of the basic CUDA functions we have written so far. For this
task, add an extra file <cite>cuda_conv.py</cite> that implements
<cite>conv2d</cite> or <cite>avgpool2d</cite>  on CUDA.</p>
</div>
<div class="section" id="task-4-5-training-an-image-classifier">
<h3>Task 4.5: Training an Image Classifier<a class="headerlink" href="#task-4-5-training-an-image-classifier" title="Permalink to this headline">Â¶</a></h3>
<p>If your code works, you should now be able to move on to the MINIST
training script in <cite>project/run_mnist_multiclass.py</cite>.  This script has the
same basic training setup as <a class="reference internal" href="module3.html"><span class="doc">Efficiency</span></a>, but now adapted to image
classification. You need to implement <cite>Conv2D</cite> and <cite>Network</cite>. The Visdom
visualization will show some hidden states of your model, like the following:</p>
<a class="reference internal image-reference" href="_images/mnist5.png"><img alt="_images/mnist5.png" class="align-center" src="_images/mnist5.png" style="width: 400px;" /></a>
<a class="reference internal image-reference" href="_images/mnist4.png"><img alt="_images/mnist4.png" class="align-center" src="_images/mnist4.png" style="width: 400px;" /></a>
<div class="admonition-todo admonition" id="id5">
<p class="admonition-title">Todo</p>
<ul class="simple">
<li><p>Train a model on MNIST, and add your training printout logs (i.e. training loss,
performance on validation set) to the README.</p></li>
<li><p>Report the Visdom visualizations of your final model's
hidden states at the end of training.</p></li>
</ul>
</div>
</div>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="cuda.html" title="previous page">GPU Programming</a>
    <a class='right-next' id="next-link" href="convolution.html" title="next page">Convolution</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020, Sasha Rush.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>