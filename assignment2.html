

<!DOCTYPE html>
<html class="writer-html5" lang="english" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Assignment &mdash; MiniTorch 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MiniTorch
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="setup.html">Workspace Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="module0.html">Module 0 - Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="module1.html">Module 1 - Auto-Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="module2.html">Module 2 - Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="module3.html">Module 3 - Efficiency</a></li>
<li class="toctree-l1"><a class="reference internal" href="compression.html">Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">Export</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn_in_js.html">Running Anywhere</a></li>
<li class="toctree-l1"><a class="reference internal" href="module4.html">Task 4.1: Model Size and Speed</a></li>
<li class="toctree-l1"><a class="reference internal" href="module4.html#task-4-2-model-compression">Task 4.2: Model Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="module4.html#task-4-3-shipping-a-model">Task 4.3: Shipping a Model</a></li>
<li class="toctree-l1"><a class="reference internal" href="module4.html#task-4-4-serving-a-model">Task 4.4: Serving a Model</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MiniTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Assignment</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/assignment2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="assignment">
<h1>Assignment<a class="headerlink" href="#assignment" title="Permalink to this headline">¶</a></h1>
<div class="section" id="tasks-2-1-tensor-data-indexing">
<h2>Tasks 2.1: Tensor Data - Indexing<a class="headerlink" href="#tasks-2-1-tensor-data-indexing" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with tensor indexing.
Be sure to first carefully read the section on
<a class="reference internal" href="tensordata.html"><span class="doc">Tensors</span></a>. You may also find it helpful to read
the tutorials for using tensors/arrays in Torch or Numpy.</p>
</div>
<p>The MiniTorch library implements the core multidimensional tensor backend as
<code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.TensorData</span></code>. This is class handles indexing, storage, transposition,
and low-level details such as strides. Before turning to the user-facing class <code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Tensor</span></code>,
first implement these core functions.</p>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Add calls in <cite>minitorch/tensor_data.py</cite> for each of the following, and pass tests marked as <cite>task2_1</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.index_to_position">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">index_to_position</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">index</span></em>, <em class="sig-param"><span class="n">strides</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.index_to_position" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a multidimensional tensor <cite>index</cite> into a single-dimensional position in
storage based on strides.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<em>tuple</em>) -- index tuple of ints</p></li>
<li><p><strong>strides</strong> (<em>tuple</em>) -- tensor strides</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position in storage</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.count">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">count</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">position</span></em>, <em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a <cite>position</cite> to an index in the <cite>shape</cite>.
Should ensure that enumerating position 0 ... size of a
tensor produces every index exactly once. It
may not be the inverse of <cite>index_to_position</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>position</strong> (<em>int</em>) -- current position</p></li>
<li><p><strong>shape</strong> (<em>tuple</em>) -- tensor shape</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an index within shape</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.TensorData.permute">
<code class="sig-prename descclassname">minitorch.TensorData.</code><code class="sig-name descname">permute</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">order</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.TensorData.permute" title="Permalink to this definition">¶</a></dt>
<dd><p>Permute the dimensions of the tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>order</strong> (<em>list</em>) -- a permutation of the dimensions</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a new TensorData with the same storage and a new dimension order.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorData</span></code></p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="tasks-2-2-tensor-operations">
<h2>Tasks 2.2: Tensor Operations<a class="headerlink" href="#tasks-2-2-tensor-operations" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with higher-order tensor operations.
Be sure to first carefully read the section on
<a class="reference internal" href="tensorops.html"><span class="doc">Operations</span></a>. You may also find it helpful to go back to
Module 0 and make sure you understand higher-order functions and currying.</p>
</div>
<p>The tensor operations will to apply high-level, higher-order
operations to all values in a tensor simultaneously. In particularly
allowing you to map, zip, and reduce tensor data objects together. On
top of this foundation we can build up a similar class to the Tensor functions
just like we did for the scalar functions. In this task you should first implement
the generic tensor ops and then use these to provide the <cite>forward</cite> functions
for tensors.</p>
<span class="target" id="module-minitorch.tensor_ops"></span><div class="admonition-todo admonition" id="id2">
<p class="admonition-title">Todo</p>
<p>Add functions in <cite>minitorch/tensor_ops.py</cite> and <cite>minitorch/tensor.py</cite> for each of the following, and pass tests marked as <cite>task2_2</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.tensor.Mul.forward">
<code class="sig-prename descclassname">minitorch.tensor.Mul.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Mul.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Sigmoid.forward">
<code class="sig-prename descclassname">minitorch.tensor.Sigmoid.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Sigmoid.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.ReLU.forward">
<code class="sig-prename descclassname">minitorch.tensor.ReLU.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.ReLU.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Log.forward">
<code class="sig-prename descclassname">minitorch.tensor.Log.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Log.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Mean.forward">
<code class="sig-prename descclassname">minitorch.tensor.Mean.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">dim</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Mean.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Copy.forward">
<code class="sig-prename descclassname">minitorch.tensor.Copy.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Copy.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.LT.forward">
<code class="sig-prename descclassname">minitorch.tensor.LT.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.LT.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="tasks-2-3-gradients-and-autograd">
<h2>Tasks 2.3: Gradients and Autograd<a class="headerlink" href="#tasks-2-3-gradients-and-autograd" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with tensor backward operations.
Be sure to first carefully read the section on
<a class="reference internal" href="tensor.html"><span class="doc">Tensor Variables</span></a>. You may also find it helpful to go back to
Module 1 and review <cite>variables</cite> and <cite>functions</cite>.</p>
</div>
<p>Just as with <a class="reference internal" href="scalar.html#minitorch.Scalar" title="minitorch.Scalar"><code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Scalar</span></code></a>, the <code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Tensor</span></code>
is a Variable that support automatic differentiation. In this task you
will need to implement each of the backward functions and ensure that
they pass the tests.</p>
<div class="admonition-todo admonition" id="id3">
<p class="admonition-title">Todo</p>
<p>Add backward functions <cite>minitorch/tensor_ops.py</cite> for each of the following, and pass tests marked as <cite>task2_3</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.tensor.Mul.backward">
<code class="sig-prename descclassname">minitorch.tensor.Mul.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Mul.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Sigmoid.backward">
<code class="sig-prename descclassname">minitorch.tensor.Sigmoid.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Sigmoid.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.ReLU.backward">
<code class="sig-prename descclassname">minitorch.tensor.ReLU.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.ReLU.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Log.backward">
<code class="sig-prename descclassname">minitorch.tensor.Log.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Log.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Mean.backward">
<code class="sig-prename descclassname">minitorch.tensor.Mean.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Mean.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Copy.backward">
<code class="sig-prename descclassname">minitorch.tensor.Copy.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Copy.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.LT.backward">
<code class="sig-prename descclassname">minitorch.tensor.LT.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.LT.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="tasks-4-tensor-broadcasting">
<h2>Tasks 4: Tensor Broadcasting<a class="headerlink" href="#tasks-4-tensor-broadcasting" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with tensor broadcasting.  Be
sure to first carefully read the section on
<a class="reference internal" href="broadcasting.html"><span class="doc">Broadcasting</span></a>. You may also find it helpful to go through
some of the tutorials of Torch or Numpy broadcasting as it is
identical.</p>
</div>
<div class="admonition-todo admonition" id="id4">
<p class="admonition-title">Todo</p>
<p>Add broadcasing functions to <cite>minitorch/tensor_data.py</cite> and <cite>minitorch/tensor_ops.py</cite> for each of the following, and pass tests marked as <cite>task2_4</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.shape_broadcast">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">shape_broadcast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape1</span></em>, <em class="sig-param"><span class="n">shape2</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.shape_broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcast two shapes to create a new union shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape1</strong> (<em>tuple</em>) -- first shape</p></li>
<li><p><strong>shape2</strong> (<em>tuple</em>) -- second shape</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>broadcasted shape</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.broadcast_index_to_position">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">broadcast_index_to_position</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">index</span></em>, <em class="sig-param"><span class="n">strides</span></em>, <em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.broadcast_index_to_position" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert an index into a position (see <cite>index_to_position</cite>),
when the index is from a broadcasted shape. In this case
it may be larger or with more dimensions then the <cite>shape</cite>
given. Additional dimensions may need to be mapped to 0 or
removeed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<em>tuple</em>) -- multidimensional index</p></li>
<li><p><strong>strides</strong> (<em>tuple</em>) -- tensor strides</p></li>
<li><p><strong>shape</strong> (<em>tuple</em>) -- tensor shape</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>storage position after unbroadcasting and converting.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.tensor_ops.get">
<code class="sig-prename descclassname">minitorch.tensor_ops.</code><code class="sig-name descname">get</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">index</span></em>, <em class="sig-param"><span class="n">stride</span></em>, <em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor_ops.get" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="tasks-5-training">
<h2>Tasks 5: Training<a class="headerlink" href="#tasks-5-training" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Read project <cite>tensor</cite></p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Sasha Rush

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>