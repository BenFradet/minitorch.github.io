

<!DOCTYPE html>
<html class="writer-html5" lang="english" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Convolution &mdash; MiniTorch 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="_static/jupyter-sphinx.css" type="text/css" />
  <link rel="stylesheet" href="_static/thebelab.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script src="_static/thebelab-helper.js"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.18.0/dist/embed-amd.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Pooling" href="pooling.html" />
    <link rel="prev" title="Module 4 - Networks" href="module4.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MiniTorch
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="setup.html">Workspace Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlprimer.html">ML Primer</a></li>
<li class="toctree-l1"><a class="reference internal" href="module0.html">Module 0 - Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="module1.html">Module 1 - Auto-Differentiation</a></li>
<li class="toctree-l1"><a class="reference internal" href="module2.html">Module 2 - Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="module3.html">Module 3 - Efficiency</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="module4.html">Module 4 - Networks</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Convolution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#d-convolution">1-D Convolution</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id1">2-D Convolution</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="pooling.html">Pooling</a></li>
<li class="toctree-l2"><a class="reference internal" href="softmax.html">Softmax</a></li>
<li class="toctree-l2"><a class="reference internal" href="module4.html#tasks">Tasks</a></li>
</ul>
</li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MiniTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="module4.html">Module 4 - Networks</a> &raquo;</li>
        
      <li>Convolution</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/convolution.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="convolution">
<h1>Convolution<a class="headerlink" href="#convolution" title="Permalink to this headline">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Convolutions are extensively covered by many excellent
tutorials. You may want to check one of these out for more details on
their use.</p>
</div>
<p>The main tool that we have used so far for classification is first the
<cite>Linear</cite> layer which applies many different linear seperators to the
input and then secondly a transform with the ReLU function into a new
hidden representation.</p>
<p>One major problem with this function is that it is based on the
<cite>absolute</cite> position of the original features. This prevents us from
using the same learned parameters on many parts of the input. Instead
we would like to have a sliding window that uses the same model on local
regions of the input.</p>
<p>Graphically, the main idea is represented by this figure:</p>
<img alt="_images/conv.png" class="align-center" src="_images/conv.png" />
<p>Instead of directly transforming the input image, this convolutional model
is applied at each part of the image to produce a new representation. It is
<cite>sliding</cite> in the sense that it slides across the whole image to produce the
output.</p>
<p>We will primarily use this convolution on images to learn these
locally applied parameters, but to first get a sense of how this
works, we will begin with sequences.</p>
<div class="section" id="d-convolution">
<h2>1-D Convolution<a class="headerlink" href="#d-convolution" title="Permalink to this headline">¶</a></h2>
<p>Our simple 1-D Convolution takes in an input vector of length T, a
weight vector of length K, and a produces an output of length T.
It computes the output by sliding a the weight along the input,
zipping it, reducing, and then saving the value in output:</p>
<img alt="_images/conv1d.png" class="align-center" src="_images/conv1d.png" />
<p>In order to make things simpler, if the weight goes over the edge we just
assume the input value was 0.</p>
<p>An alternative way to think about a convolution is as <cite>unrolling</cite> the input.</p>
<blockquote>
<div><p>input = minitorch.tensor([1, 2, 3, 4, 5, 6])
weight = minitorch.tensor([5, 2, 3])</p>
</div></blockquote>
<p>Now imagine we have a function <cite>unroll</cite> that could take a tensor and
produce a new tensor</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">unroll</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
 <span class="p">]</span>
</pre></div>
</div>
<p>We then apply matrix multiplication to take the dot-product</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="nb">input</span> <span class="o">@</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

<span class="p">[</span><span class="mi">18</span><span class="p">,</span> <span class="o">...</span> <span class="p">]</span>
</pre></div>
</div>
<p>We can view this as two separate operations (note though that we do
not implement it this way!):</p>
<img alt="_images/convvec.png" class="align-center" src="_images/convvec.png" />
<p>A neural network convolution is the fusion of these two
operations. Given a tensor input and weight, it efficiently unrolls A
and takes the matrix multiplication.  This technique is really useful
because it allows you to <cite>hover</cite> over a region of the source
sentence. You can think of the weight matrix as applying a pattern (or
many different patterns) that you trying to find in the original data
input.</p>
<p>As with every other operation we need to be able to compute the
backward operations of this 1D convolution. Also as with matrix
multiplication we can reason through the flow of each cell. In
particular we not that the third cell in the input <cite>input[2]</cite> was
utilized only to compute three different cells in the output,
<cite>output[0]</cite>, <cite>output[1]</cite> and <cite>output[2]</cite></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="o">+</span> <span class="n">weight</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
<span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>  <span class="o">+</span> <span class="n">weight</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
<span class="n">output</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="n">weight</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>  <span class="o">+</span> <span class="n">weight</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="nb">input</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
</pre></div>
</div>
<p>Therefore when calculating gradients we can see that:</p>
<blockquote>
<div><p>grad_input[2] = weight[0] * grad_output[0] + weight[1] * grad_output[1]  + weight[2] * grad_output[2]</p>
</div></blockquote>
<p>Visually this implies that the backward of the convolution is a convolution anchored in the reverse direction:</p>
<img alt="_images/conv1dback.png" class="align-center" src="_images/conv1dback.png" />
<p>Similar updates can be derived for the gradient of the weight.</p>
<img alt="_images/conv1dback2.png" class="align-center" src="_images/conv1dback2.png" />
<p>This implies, as with matrix multiplication, that implementing a fast convolution can be used for both forward and backward passes.</p>
<p>Finally, this whole approach can be scaled up to allow for handling
multiple inputs and multiple weights simultaneously through
channels. Analogously to matrix multiplication, we take an input
matrix of in_channels x T, a weight matrix of out_channels x
in_channels x K and produce an output of out_channels x T. Here is
what that looks like with in_channels = 2 and out_channels = 3.</p>
<img alt="_images/channels.png" class="align-center" src="_images/channels.png" />
<p>Codewise this is what the function computes. (It is a bit order to
read so I recommend just sticking with the diagram)</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">minitorch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">in_channels</span><span class="p">,</span> <span class="n">T</span><span class="p">)</span>
<span class="n">weight</span> <span class="o">=</span> <span class="n">minitorch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">unroll</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="c1"># Shape  in_channels x K x T</span>
<span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">out_channels</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">*</span> <span class="n">K</span><span class="p">)</span> <span class="o">@</span> <span class="nb">input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">in_channels</span> <span class="o">*</span> <span class="n">K</span><span class="p">,</span> <span class="n">T</span><span class="p">))</span>
</pre></div>
</div>
<p>The 1D convolution has all sorts of neat applications. It can be
applied in NLP as a way of applying a model to multiple neighboring
words. It can be used in speech recognition as a way of recognizing
important sounds. It can be used in anomoly detection to find patterns
that trigger an alert. Anywhere you can imagine that a mini-neural
network might be useful to apply to subset of a sequence, try out a 1D
conv.</p>
</div>
<div class="section" id="id1">
<h2>2-D Convolution<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>While 1D Convolutions detect patterns along a sequence, 2D
convolutions detect patterns within a grid.</p>
<img alt="_images/conv.png" class="align-center" src="_images/conv.png" />
<p>The underlying math for the simple 2D is very similar to the 1D case.
Our simple 2-D Convolution takes in an input vector of shape H x W
(height and width), a weight vector of shape KH x KW, and a produces
an output of length H x W. The operation is nearly identical, we walk
through each possible unrolled rectangle in the input matrix and apply
the weight as a dot-product. Assuming we had an analogous unroll
function for matrices, this would be equivalent to computing</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">unrolled_input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">KH</span> <span class="o">*</span> <span class="n">KW</span><span class="p">)</span> <span class="o">@</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">KH</span> <span class="o">*</span> <span class="n">KW</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</pre></div>
</div>
<p>Another way to think about this is just applying weight as a Linear
layer to each one of the rectangles in the input image.</p>
<p>Critically, just as the 1D convolution was anchored at the left
position. the 2D convolution will be anchorsed at the top-left
position. To compute the backward computation, we compute a
bottom-right reverse convolution:</p>
<img alt="_images/backward.png" class="align-center" src="_images/backward.png" />
<p>Finally, we can again complicate things by applying many weights to
many inputs simultaneously. This extension gives us the standard 2D
convolution that is used in Torch.  We take an input matrix of
in_channels x H x W, a weight matrix of out_channels x in_channels x
KW x KH and produce an output of out_channels x H x W.</p>
<img alt="_images/conv2.png" class="align-center" src="_images/conv2.png" />
<p>Very roughly, this gives an output that takes the form</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">output</span> <span class="o">=</span> <span class="p">(</span><span class="n">unrolled_input</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">in_channels</span> <span class="o">*</span> <span class="n">KH</span> <span class="o">*</span> <span class="n">KW</span><span class="p">)</span> \
         <span class="o">@</span> <span class="n">weight</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">in_channels</span> <span class="o">*</span> <span class="n">KH</span> <span class="o">*</span> <span class="n">KW</span><span class="p">,</span> <span class="n">out_channels</span><span class="p">))</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>
</pre></div>
</div>
<p>The 2D Convolution is the main operator for image recognition
systems. It allows us to process images into local feature
representations:</p>
<img alt="_images/im1.png" class="align-center" src="_images/im1.png" />
<img alt="_images/im2.png" class="align-center" src="_images/im2.png" />
<p>It is also the key step in the convolutional neural network
pipeline. It transforms the input image into hidden features which get
propagated through each stage of the network:</p>
<img alt="_images/networkcnn.png" class="align-center" src="_images/networkcnn.png" />
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="pooling.html" class="btn btn-neutral float-right" title="Pooling" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="module4.html" class="btn btn-neutral float-left" title="Module 4 - Networks" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Sasha Rush

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>