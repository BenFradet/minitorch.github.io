
<!DOCTYPE html>

<html lang="english">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Backpropagation &#8212; MiniTorch 0.1 documentation</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/basic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/thebelab.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/thebelab-helper.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Module 2 - Tensors" href="module2.html" />
    <link rel="prev" title="Autodifferentiation" href="chainrule.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    
<a class="navbar-brand" href="index.html">
<p class="title">MiniTorch</p>
</a>

    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="setup.html">
  Workspace Setup
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="contributing.html">
  Contributing
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="mlprimer.html">
  ML Primer
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module0.html">
  Module 0 - Fundamentals
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="module1.html">
  Module 1 - Auto-Differentiation
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module2.html">
  Module 2 - Tensors
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module3.html">
  Module 3 - Efficiency
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module4.html">
  Module 4 - Networks
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p class="caption">
 <span class="caption-text">
  Guides
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="derivative.html">
   Derivatives
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="scalar.html">
   Tracking Variables
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chainrule.html">
   Autodifferentiation
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Backpropagation
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                
<div class="tocsection onthispage pt-5 pb-3">
    <i class="fas fa-list"></i> On this page
</div>

<nav id="bd-toc-nav">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#example">
   Example
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algorithm">
   Algorithm
  </a>
 </li>
</ul>

</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <div class="section" id="backpropagation">
<h1>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this headline">¶</a></h1>
<p>The <cite>backward</cite> function tells us how to compute the derivative of one
operation.  The chain rule tells us how to compute the derivative of two
sequential operations.  In this section, we show how to use these to
compute the derivative for an arbitrary series of operations.</p>
<p>The underlying approach we will use is a breadth-first search over the
computation
graph constructed by Variables and Functions. Before going over the algorithm,
let's work through a specific example step by step.</p>
<div class="section" id="example">
<h2>Example<a class="headerlink" href="#example" title="Permalink to this headline">¶</a></h2>
<p>Assume we have Variables <span class="math notranslate nohighlight">\(x,y\)</span> and a Function <span class="math notranslate nohighlight">\(h(x,y)\)</span>. We want
to compute
the derivatives <span class="math notranslate nohighlight">\(h'_x(x, y)\)</span> and <span class="math notranslate nohighlight">\(h'_y(x, y)\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{eqnarray*}
z &amp;=&amp; x \times y \\
h(x, y) &amp;=&amp; \log(z) + \exp(z)
\end{eqnarray*}\end{split}\]</div>
<p>We assume x, +, log, and exp are all implemented as ScalarFunctions which
can store
their history. This means that the final output Variable has constructed a
graph of its history
that looks like this:</p>
<img alt="_images/backprop1.png" class="align-center" src="_images/backprop1.png" />
<p>Here, starting from the left, the arrows represent Variables <span class="math notranslate nohighlight">\(x,y\)</span>,
then <span class="math notranslate nohighlight">\(z, z\)</span>, then <span class="math notranslate nohighlight">\(\log(z), \exp(z)\)</span>, and finally <span class="math notranslate nohighlight">\(h(x,
y)\)</span>. Forward computation proceeds left-to-right.</p>
<p>The chain rule tells us how to compute the derivatives. We need to apply
the <cite>backward</cite> functions right-to-left until we reach the input Variables
<span class="math notranslate nohighlight">\(x,y\)</span>, which we call <cite>leaf</cite> Variables. We do this by maintaining
a queue of active Variables to process. At each step, we pull a Variable
from the queue, apply the chain rule to the last Function that acted on it,
and then put its input Variables into the queue.</p>
<p>We start with only the last Variable <span class="math notranslate nohighlight">\(h(x,y)\)</span> in the queue (red arrow
in the graph below). By default, its derivative is 1.</p>
<img alt="_images/backprop2.png" class="align-center" src="_images/backprop2.png" />
<p>We then process it with the chain rule. This calls the <cite>backward</cite> function
of +, and adds
two Variables to the queue (which correspond to <span class="math notranslate nohighlight">\(\log(z), \exp(z)\)</span>
from the <cite>forward</cite> pass).</p>
<img alt="_images/backprop3.png" class="align-center" src="_images/backprop3.png" />
<p>The next Variable in the queue is the top red arrow in the above graph. We
pass its derivative as <span class="math notranslate nohighlight">\(d_{out}\)</span> in the chain rule,
which adds a Variable (corresponding to <span class="math notranslate nohighlight">\(z\)</span>: left red arrow below)
to the queue.</p>
<img alt="_images/backprop4.png" class="align-center" src="_images/backprop4.png" />
<p>The next Variable in the queue is the bottown red arrow in the above
graph. Here we have an interesting result. We have a new arrow, but it
corresponds to the same Variable which is already in the queue.
It is fine to have the Variable twice.
Alternatively we can apply a code optimization: simply add its derivative
computed at this step to its derivative computed last time.
This means we only need to process one Variable in the queue.</p>
<img alt="_images/backprop5.png" class="align-center" src="_images/backprop5.png" />
<p>After working on this Varaible, at this point, all that is left in the queue
is our leaf Variables.</p>
<img alt="_images/backprop6.png" class="align-center" src="_images/backprop6.png" />
<p>We then pull a Variable from the queue that represents an orginal leaf node,
<span class="math notranslate nohighlight">\(x\)</span>.
Since each step of this process is an application of the chain rule, we can
show that this final value
is <span class="math notranslate nohighlight">\(h'_x(x, y)\)</span>. The next and last step is to compute <span class="math notranslate nohighlight">\(h'_y(x, y)\)</span>.</p>
<img alt="_images/backprop7.png" class="align-center" src="_images/backprop7.png" />
<p>By convention, if <span class="math notranslate nohighlight">\(x, y\)</span> are instances of  <a class="reference internal" href="scalar.html#minitorch.Variable" title="minitorch.Variable"><code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Variable</span></code></a>,
their derivatives are stored as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x</span><span class="o">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">derivative</span>
</pre></div>
</div>
</div>
<div class="section" id="algorithm">
<h2>Algorithm<a class="headerlink" href="#algorithm" title="Permalink to this headline">¶</a></h2>
<p>This algorithm is an instance of a classic graph algorithm: <a class="reference external" href="https://en.wikipedia.org/wiki/Breadth-first_search">breadth-first
search</a>.</p>
<p>As illustrated in the graph for the above example,  each of the red arrows
represents
an object <code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.VariableWithDeriv</span></code>, which stores the Variable
and its current derivative
(which gets passed to <span class="math notranslate nohighlight">\(d_{out}\)</span> in the chain rule).
Starting from the rightmost arrow, which is passed in as an argument,
backpropagate should run the following algorithm:</p>
<ol class="arabic simple" start="0">
<li><p>Initialize a queue with the final Variable+derivative</p></li>
<li><p>While the queue is not empty, pull a Variable+derivative from the queue:</p>
<ol class="loweralpha simple">
<li><p>if the Variable is a leaf, add its final derivative (<cite>_add_deriv</cite>)
and loop to (1)</p></li>
<li><p>if the Variable is not a leaf,</p>
<ol class="arabic simple">
<li><p>call <cite>.chain_rule</cite> on the last function that created it with
derivative as <span class="math notranslate nohighlight">\(d_{out}\)</span></p></li>
<li><p>loop through all the Variables+derivative produced by the chain
rule (removing constants)</p></li>
<li><p>optional, if the Variable is in the queue (check <cite>.name</cite>),
add to its current derivativ;</p></li>
<li><p>otherwise, add to the queue.</p></li>
</ol>
</li>
</ol>
</li>
</ol>
<p>Important note: only leaf Variables should ever have non-None
<cite>.derivative</cite> value. All intermediate Variables should only keep
their current derivative values in the queue.</p>
</div>
</div>


              </div>
              
              
              <div class='prev-next-bottom'>
                
    <a class='left-prev' id="prev-link" href="chainrule.html" title="previous page">Autodifferentiation</a>
    <a class='right-next' id="next-link" href="module2.html" title="next page">Module 2 - Tensors</a>

              </div>
              
          </main>
          

      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  <footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020, Sasha Rush.<br/>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.0.<br/>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>