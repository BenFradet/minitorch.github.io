
<!DOCTYPE html>

<html lang="english">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Tensor Variables &#8212; MiniTorch 0.1 documentation</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/css/blank.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/jupyter-sphinx.css" />
    <link rel="stylesheet" type="text/css" href="_static/thebelab.css" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <script src="_static/thebelab-helper.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.0/dist/embed-amd.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Efficiency" href="module3.html" />
    <link rel="prev" title="Operations" href="tensorops.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="english">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    
    <nav class="navbar navbar-light navbar-expand-lg bg-light fixed-top bd-navbar" id="navbar-main"><div class="container-xl">

  <div id="navbar-start">
    
    

<a class="navbar-brand" href="index.html">
  <img src="_static/minitorch.svg" class="logo" alt="logo">
</a>


    
  </div>

  <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-collapsible" aria-controls="navbar-collapsible" aria-expanded="false" aria-label="Toggle navigation">
    <span class="navbar-toggler-icon"></span>
  </button>

  
  <div id="navbar-collapsible" class="col-lg-9 collapse navbar-collapse">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <ul id="navbar-main-elements" class="navbar-nav">
    <li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="setup.html">
  Setup
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="mlprimer.html">
  ML Primer
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module0.html">
  Fundamentals
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module1.html">
  Autodiff
 </a>
</li>

<li class="toctree-l1 current active nav-item">
 <a class="reference internal nav-link" href="module2.html">
  Tensors
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module3.html">
  Efficiency
 </a>
</li>

<li class="toctree-l1 nav-item">
 <a class="reference internal nav-link" href="module4.html">
  Networks
 </a>
</li>

    
</ul>
      </div>
      
    </div>

    <div id="navbar-end">
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
        <li class="nav-item">
          <a class="nav-link" href="https://github.com/minitorch/" rel="noopener" target="_blank" title="GitHub">
            <span><i class="fab fa-github-square"></i></span>
            <label class="sr-only">GitHub</label>
          </a>
        </li>
        <li class="nav-item">
          <a class="nav-link" href="https://twitter.com/srush_nlp" rel="noopener" target="_blank" title="Twitter">
            <span><i class="fab fa-twitter-square"></i></span>
            <label class="sr-only">Twitter</label>
          </a>
        </li>
      </ul>
      </div>
      
    </div>
  </div>
</div>
    </nav>
    

    <div class="container-xl">
      <div class="row">
          
            
            <!-- Only show if we have sidebars configured, else just a small margin  -->
            <div class="col-12 col-md-3 bd-sidebar"><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
  <div class="bd-toc-item active">
    <p>
 <span class="caption-text">
  Guides
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="tensordata.html">
   Tensors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="broadcasting.html">
   Broadcasting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tensorops.html">
   Operations
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Tensor Variables
  </a>
 </li>
</ul>

  </div>
</nav>
            </div>
            
          

          
          <div class="d-none d-xl-block col-xl-2 bd-toc">
            
              
              <div class="toc-item">
                

<nav id="bd-toc-nav">
    
</nav>
              </div>
              
              <div class="toc-item">
                
              </div>
              
            
          </div>
          

          
          
            
          
          <main class="col-12 col-md-9 col-xl-7 py-md-5 pl-md-5 pr-md-4 bd-content" role="main">
              
              <div>
                
  <section id="tensor-variables">
<h1>Tensor Variables<a class="headerlink" href="#tensor-variables" title="Permalink to this headline">Â¶</a></h1>
<p>Next, we consider autodifferentiation in the tensor framework. We have
now moved from scalars and derivatives to vectors, matrices, and
tensors.  This means <cite>multivariate calculus</cite> can bring into play
somewhat scary terminology. However, most of what we actually does not
require complicated terminology or much technical math. In
fact, except some name changes, we have already built almost
everything we need in <a class="reference internal" href="module1.html"><span class="doc">Autodiff</span></a>.</p>
<p>The key idea is, just as we had <cite>Scalar</cite> and <cite>ScalarFunction</cite>, we need to
construct <cite>Tensor</cite> and <cite>TensorFunction</cite> (which we just call <cite>Function</cite>).
These new objects behave very similar to their counterparts:</p>
<p>a) Tensors cannot be operated on directly, but need to be transformed through
a Function.
b) Functions must implement both <cite>forward</cite> and <cite>backward</cite>.
c) These transformations are tracked, which allow backpropagation through
the chain rule.</p>
<p>All of this machinery should work out of the box.</p>
<p>The main new terminology to know is <cite>gradient</cite>. Just as a tensor is a
multidimensional array of scalars, a gradient is a multidimensional
array of derivatives for these scalars. Consider the following code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Assignment 1 notation</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="n">a</span><span class="o">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">b</span><span class="o">.</span><span class="n">derivative</span><span class="p">,</span> <span class="n">c</span><span class="o">.</span><span class="n">derivative</span>


<span class="c1"># Assignment 2 notation</span>
<span class="n">tensor1</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">g</span><span class="p">(</span><span class="n">tensor1</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="c1"># shape (3,)</span>
<span class="n">tensor1</span><span class="o">.</span><span class="n">grad</span>
</pre></div>
</div>
<p>The gradient of <cite>tensor1</cite> is a tensor that holds the derivatives of
each of its elements. Another place that gradients come into play is
that <cite>backward</cite> no longer takes <span class="math notranslate nohighlight">\(d_{out}\)</span> as an argument, but now
takes <span class="math notranslate nohighlight">\(grad_{out}\)</span> which is just a tensor consisting of all the
<span class="math notranslate nohighlight">\(d_{out}\)</span>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You will find lots of different notation for
gradients and multivariate terminology. For this Module, you are
supposed to ignore it and stick to everything you know about derivatives.
It turns out that you can do most of machine learning without ever
thinking in higher dimensions.</p>
</div>
<p>If you think about gradient and <span class="math notranslate nohighlight">\(grad_{out}\)</span> in this way
(i.e. tensors of derivatives and <span class="math notranslate nohighlight">\(d_{out}\)</span>),
then you can see how we can easily compute the gradient for tensor
operations using univariate rules.</p>
<ol class="arabic simple">
<li><p><strong>map</strong>. Given a tensor, <cite>map</cite> applies a univariate operation to each scalar
position individually. For a scalar <span class="math notranslate nohighlight">\(x\)</span>, consider computing
<span class="math notranslate nohighlight">\(g(x)\)</span>.  From Module 1, we know that the
derivative of <span class="math notranslate nohighlight">\(f(g(x))\)</span> is equal to <span class="math notranslate nohighlight">\(g'(x) \times d_{out}\)</span>.
To compute the gradient in <cite>backward</cite>, we only need to compute the
derivative for each scalar position and then apply a <cite>mul</cite> map.</p></li>
</ol>
<img alt="_images/map back.png" class="align-center" src="_images/map back.png" />
<ol class="arabic simple" start="2">
<li><p><strong>zip</strong>. Given two tensors, <cite>zip</cite> applies a binary operation to each
pair of scalars. For two scalars <span class="math notranslate nohighlight">\(x\)</span> and
<span class="math notranslate nohighlight">\(y\)</span>, consider computing <span class="math notranslate nohighlight">\(g(x, y)\)</span>.  From Module
1, we know that the derivative of <span class="math notranslate nohighlight">\(f(g(x, y))\)</span> is equal to
<span class="math notranslate nohighlight">\(g_x'(x, y) \times d_{out}\)</span> and <span class="math notranslate nohighlight">\(g_y'(x, y) \times d_{out}\)</span>.
Thus to compute the gradient, we only need to compute the
derivative for each scalar position and apply a <cite>mul</cite> map.</p></li>
</ol>
<img alt="_images/zip back.png" class="align-center" src="_images/zip back.png" />
<ol class="arabic simple" start="3">
<li><p><strong>reduce</strong>. Given a tensor, <cite>reduce</cite> applies an aggregation
operation to one dimension. For simplicity, let's consider sum-based
reductions.  For scalars <span class="math notranslate nohighlight">\(x_1\)</span> to <span class="math notranslate nohighlight">\(x_n\)</span>, consider computing
<span class="math notranslate nohighlight">\(x_1 + x_2 + \ldots + x_n\)</span>.  For any <span class="math notranslate nohighlight">\(x_i\)</span> value, the derivative
is 1.
Therefore, the derivative for any position computed in <cite>backward</cite> is simply
<span class="math notranslate nohighlight">\(d_{out}\)</span>. This means to compute the
gradient, we only need to send <span class="math notranslate nohighlight">\(d_{out}\)</span> to each
position. (For other reduce operations such as <cite>product</cite>, you get
different expansions, which can be calculated just by taking
derivatives).</p></li>
</ol>
<img alt="_images/reduce back.png" class="align-center" src="_images/reduce back.png" />
</section>


              </div>
              
              
              <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="tensorops.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Operations</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="module3.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Efficiency</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
              
          </main>
          

      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>
<footer class="footer mt-5 mt-md-0">
  <div class="container">
    
    <div class="footer-item">
      <p class="copyright">
    &copy; Copyright 2020, Sasha Rush.<br>
</p>
    </div>
    
    <div class="footer-item">
      <p class="sphinx-version">
Created using <a href="http://sphinx-doc.org/">Sphinx</a> 3.2.0.<br>
</p>
    </div>
    
  </div>
</footer>
  </body>
</html>