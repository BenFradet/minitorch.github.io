

<!DOCTYPE html>
<html class="writer-html5" lang="english" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Module 2 - Tensors &mdash; MiniTorch 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Tensors" href="tensordata.html" />
    <link rel="prev" title="Backpropagate" href="backpropagate.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MiniTorch
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="setup.html">Workspace Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="module0.html">Module 0 - Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="module1.html">Module 1 - Auto-Differentiation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Module 2 - Tensors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tensordata.html">Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorops.html">Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor.html">Tensor Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="broadcasting.html">Broadcasting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tasks">Tasks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tasks-2-1-tensor-data-indexing">Tasks 2.1: Tensor Data - Indexing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tasks-2-2-tensor-operations">Tasks 2.2: Tensor Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tasks-2-3-gradients-and-autograd">Tasks 2.3: Gradients and Autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tasks-2-4-tensor-broadcasting">Tasks 2.4: Tensor Broadcasting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-2-5-training">Task 2.5: Training</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module3.html">Module 3 - Efficiency</a></li>
<li class="toctree-l1"><a class="reference internal" href="compression.html">Compression</a></li>
<li class="toctree-l1"><a class="reference internal" href="onnx.html">Export</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn_in_js.html">Running Anywhere</a></li>
<li class="toctree-l1"><a class="reference internal" href="module4.html">Tasks</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MiniTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Module 2 - Tensors</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/module2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-2-tensors">
<h1>Module 2 - Tensors<a class="headerlink" href="#module-2-tensors" title="Permalink to this headline">¶</a></h1>
<img alt="_images/stride4.png" class="align-center" src="_images/stride4.png" />
<p>We now have a fully developed autodifferentiation system built around
scalars. This system is correct, but it very inefficient. All numbers
are wrap with an object, and each operation requires storing a graph of
all the values that we had previously created. Training requires
repeating these operations, and computing modules such as linear required
a <cite>for</cite> loop over each of the terms in our network.</p>
<p>This module introduces and implements a <strong>tensor</strong> object that will
solve all of these problems. Tensors all of to group together many of
these repeated operations to save python overhead and to pass off
grouped operations to faster implementations.</p>
<p>All starter code is available in <a class="reference external" href="https://github.com/minitorch/Module-2">https://github.com/minitorch/Module-2</a> .</p>
<p>To begin this module, remember to first activate your virtual environment.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">source</span> <span class="n">venv</span><span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">activate</span>
</pre></div>
</div>
<p>And then clone your assignment.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">git</span> <span class="n">clone</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT2_URL</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cd</span> <span class="n">minitorch2</span>
</pre></div>
</div>
<p>You will need the files from Assignment 0 so be sure to pull them over
to your new repo.</p>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tensordata.html">Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorops.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="broadcasting.html">Broadcasting</a></li>
</ul>
</div>
<div class="section" id="tasks">
<h2>Tasks<a class="headerlink" href="#tasks" title="Permalink to this headline">¶</a></h2>
<div class="section" id="tasks-2-1-tensor-data-indexing">
<h3>Tasks 2.1: Tensor Data - Indexing<a class="headerlink" href="#tasks-2-1-tensor-data-indexing" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with tensor indexing.
Be sure to first carefully read the section on
<a class="reference internal" href="tensordata.html"><span class="doc">Tensors</span></a>. You may also find it helpful to read
the tutorials for using tensors/arrays in Torch or Numpy.</p>
</div>
<p>The MiniTorch library implements the core multidimensional tensor backend as
<code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.TensorData</span></code>. This is class handles indexing, storage, transposition,
and low-level details such as strides. Before turning to the user-facing class <code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Tensor</span></code>,
first implement these core functions.</p>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Add calls in <cite>minitorch/tensor_data.py</cite> for each of the following, and pass tests marked as <cite>task2_1</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.index_to_position">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">index_to_position</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">index</span></em>, <em class="sig-param"><span class="n">strides</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.index_to_position" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts a multidimensional tensor <cite>index</cite> into a single-dimensional position in
storage based on strides.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<em>tuple</em>) -- index tuple of ints</p></li>
<li><p><strong>strides</strong> (<em>tuple</em>) -- tensor strides</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>position in storage</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.count">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">count</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">position</span></em>, <em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a <cite>position</cite> to an index in the <cite>shape</cite>.
Should ensure that enumerating position 0 ... size of a
tensor produces every index exactly once. It
may not be the inverse of <cite>index_to_position</cite>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>position</strong> (<em>int</em>) -- current position</p></li>
<li><p><strong>shape</strong> (<em>tuple</em>) -- tensor shape</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>an index within shape</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>list</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.TensorData.permute">
<code class="sig-prename descclassname">minitorch.TensorData.</code><code class="sig-name descname">permute</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">order</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.TensorData.permute" title="Permalink to this definition">¶</a></dt>
<dd><p>Permute the dimensions of the tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>order</strong> (<em>list</em>) -- a permutation of the dimensions</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a new TensorData with the same storage and a new dimension order.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorData</span></code></p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="tasks-2-2-tensor-operations">
<h3>Tasks 2.2: Tensor Operations<a class="headerlink" href="#tasks-2-2-tensor-operations" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with higher-order tensor operations.
Be sure to first carefully read the section on
<a class="reference internal" href="tensorops.html"><span class="doc">Operations</span></a>. You may also find it helpful to go back to
Module 0 and make sure you understand higher-order functions and currying.</p>
</div>
<p>The tensor operations will to apply high-level, higher-order
operations to all values in a tensor simultaneously. In particularly
allowing you to map, zip, and reduce tensor data objects together. On
top of this foundation we can build up a similar class to the Tensor functions
just like we did for the scalar functions. In this task you should first implement
the generic tensor ops and then use these to provide the <cite>forward</cite> functions
for tensors.</p>
<span class="target" id="module-minitorch.tensor_ops"></span><div class="admonition-todo admonition" id="id2">
<p class="admonition-title">Todo</p>
<p>Add functions in <cite>minitorch/tensor_ops.py</cite> and <cite>minitorch/tensor.py</cite> for each of the following, and pass tests marked as <cite>task2_2</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.tensor.Mul.forward">
<code class="sig-prename descclassname">minitorch.tensor.Mul.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Mul.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Sigmoid.forward">
<code class="sig-prename descclassname">minitorch.tensor.Sigmoid.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Sigmoid.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.ReLU.forward">
<code class="sig-prename descclassname">minitorch.tensor.ReLU.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.ReLU.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Log.forward">
<code class="sig-prename descclassname">minitorch.tensor.Log.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Log.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Mean.forward">
<code class="sig-prename descclassname">minitorch.tensor.Mean.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">dim</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Mean.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Copy.forward">
<code class="sig-prename descclassname">minitorch.tensor.Copy.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Copy.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.LT.forward">
<code class="sig-prename descclassname">minitorch.tensor.LT.</code><code class="sig-name descname">forward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">a</span></em>, <em class="sig-param"><span class="n">b</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.LT.forward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="tasks-2-3-gradients-and-autograd">
<h3>Tasks 2.3: Gradients and Autograd<a class="headerlink" href="#tasks-2-3-gradients-and-autograd" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with tensor backward operations.
Be sure to first carefully read the section on
<a class="reference internal" href="tensor.html"><span class="doc">Tensor Variables</span></a>. You may also find it helpful to go back to
Module 1 and review <cite>variables</cite> and <cite>functions</cite>.</p>
</div>
<p>Just as with <a class="reference internal" href="scalar.html#minitorch.Scalar" title="minitorch.Scalar"><code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Scalar</span></code></a>, the <code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Tensor</span></code>
is a Variable that support automatic differentiation. In this task you
will need to implement each of the backward functions and ensure that
they pass the tests.</p>
<div class="admonition-todo admonition" id="id3">
<p class="admonition-title">Todo</p>
<p>Add backward functions <cite>minitorch/tensor_ops.py</cite> for each of the following, and pass tests marked as <cite>task2_3</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.tensor.Mul.backward">
<code class="sig-prename descclassname">minitorch.tensor.Mul.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Mul.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Sigmoid.backward">
<code class="sig-prename descclassname">minitorch.tensor.Sigmoid.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Sigmoid.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.ReLU.backward">
<code class="sig-prename descclassname">minitorch.tensor.ReLU.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.ReLU.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Log.backward">
<code class="sig-prename descclassname">minitorch.tensor.Log.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Log.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Mean.backward">
<code class="sig-prename descclassname">minitorch.tensor.Mean.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Mean.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.Copy.backward">
<code class="sig-prename descclassname">minitorch.tensor.Copy.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.Copy.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="py function">
<dt id="minitorch.tensor.LT.backward">
<code class="sig-prename descclassname">minitorch.tensor.LT.</code><code class="sig-name descname">backward</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">ctx</span></em>, <em class="sig-param"><span class="n">grad_output</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor.LT.backward" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="tasks-2-4-tensor-broadcasting">
<h3>Tasks 2.4: Tensor Broadcasting<a class="headerlink" href="#tasks-2-4-tensor-broadcasting" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with tensor broadcasting.  Be
sure to first carefully read the section on
<a class="reference internal" href="broadcasting.html"><span class="doc">Broadcasting</span></a>. You may also find it helpful to go through
some of the tutorials of Torch or Numpy broadcasting as it is
identical.</p>
</div>
<div class="admonition-todo admonition" id="id4">
<p class="admonition-title">Todo</p>
<p>Add broadcasing functions to <cite>minitorch/tensor_data.py</cite> and <cite>minitorch/tensor_ops.py</cite> for each of the following, and pass tests marked as <cite>task2_4</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.shape_broadcast">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">shape_broadcast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape1</span></em>, <em class="sig-param"><span class="n">shape2</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.shape_broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcast two shapes to create a new union shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape1</strong> (<em>tuple</em>) -- first shape</p></li>
<li><p><strong>shape2</strong> (<em>tuple</em>) -- second shape</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>broadcasted shape</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.broadcast_index_to_position">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">broadcast_index_to_position</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">index</span></em>, <em class="sig-param"><span class="n">strides</span></em>, <em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.broadcast_index_to_position" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert an index into a position (see <cite>index_to_position</cite>),
when the index is from a broadcasted shape. In this case
it may be larger or with more dimensions then the <cite>shape</cite>
given. Additional dimensions may need to be mapped to 0 or
removeed.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>index</strong> (<em>tuple</em>) -- multidimensional index</p></li>
<li><p><strong>strides</strong> (<em>tuple</em>) -- tensor strides</p></li>
<li><p><strong>shape</strong> (<em>tuple</em>) -- tensor shape</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>storage position after unbroadcasting and converting.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>int</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.tensor_ops.get">
<code class="sig-prename descclassname">minitorch.tensor_ops.</code><code class="sig-name descname">get</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">index</span></em>, <em class="sig-param"><span class="n">stride</span></em>, <em class="sig-param"><span class="n">shape</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor_ops.get" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

</div>
<div class="section" id="task-2-5-training">
<h3>Task 2.5: Training<a class="headerlink" href="#task-2-5-training" title="Permalink to this headline">¶</a></h3>
<p>If your code works you should now be able to run the tensor training script in <cite>project/run_tensor.py</cite>.</p>
<div class="admonition-todo admonition" id="id5">
<p class="admonition-title">Todo</p>
<p>Train a tensor model and add you results to the README.</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tensordata.html" class="btn btn-neutral float-right" title="Tensors" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="backpropagate.html" class="btn btn-neutral float-left" title="Backpropagate" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Sasha Rush

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>