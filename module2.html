

<!DOCTYPE html>
<html class="writer-html5" lang="english" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Module 2 - Tensors &mdash; MiniTorch 0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Tensors" href="tensordata.html" />
    <link rel="prev" title="Backpropagate" href="backpropagate.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> MiniTorch
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="setup.html">Workspace Setup</a></li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a></li>
<li class="toctree-l1"><a class="reference internal" href="module0.html">Module 0 - Fundamentals</a></li>
<li class="toctree-l1"><a class="reference internal" href="module1.html">Module 1 - Auto-Differentiation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Module 2 - Tensors</a><ul>
<li class="toctree-l2"><a class="reference internal" href="tensordata.html">Tensors</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensorops.html">Operations</a></li>
<li class="toctree-l2"><a class="reference internal" href="tensor.html">Tensor Variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="broadcasting.html">Broadcasting</a></li>
<li class="toctree-l2"><a class="reference internal" href="#tasks">Tasks</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tasks-2-1-tensor-data-indexing">Tasks 2.1: Tensor Data - Indexing</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tasks-2-2-tensor-operations">Tasks 2.2: Tensor Operations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tasks-2-3-gradients-and-autograd">Tasks 2.3: Gradients and Autograd</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tasks-2-4-tensor-broadcasting">Tasks 2.4: Tensor Broadcasting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#task-2-5-training">Task 2.5: Training</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="module3.html">Module 3 - Efficiency</a></li>
<li class="toctree-l1"><a class="reference internal" href="module4.html">Module 4 - Inference</a></li>
<li class="toctree-l1"><a class="reference internal" href="mlprimer.html">ML Primer</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">MiniTorch</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Module 2 - Tensors</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/module2.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="module-2-tensors">
<h1>Module 2 - Tensors<a class="headerlink" href="#module-2-tensors" title="Permalink to this headline">¶</a></h1>
<img alt="_images/stride4.png" class="align-center" src="_images/stride4.png" />
<p>We now have a fully developed autodifferentiation system built around
scalars. This system is correct, but it very inefficient. All numbers
are wrap with an object, and each operation requires storing a graph of
all the values that we had previously created. Training requires
repeating these operations, and computing modules such as linear required
a <cite>for</cite> loop over each of the terms in our network.</p>
<p>This module introduces and implements a <strong>tensor</strong> object that will
solve all of these problems. Tensors all of to group together many of
these repeated operations to save python overhead and to pass off
grouped operations to faster implementations.</p>
<p>All starter code is available in <a class="reference external" href="https://github.com/minitorch/Module-2">https://github.com/minitorch/Module-2</a> .</p>
<p>To begin this module, remember to first activate your virtual environment.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">source</span> <span class="n">venv</span><span class="o">/</span><span class="nb">bin</span><span class="o">/</span><span class="n">activate</span>
</pre></div>
</div>
<p>And then clone your assignment.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">git</span> <span class="n">clone</span> <span class="p">{{</span><span class="n">STUDENT_ASSIGNMENT2_URL</span><span class="p">}}</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cd</span> <span class="n">minitorch2</span>
</pre></div>
</div>
<p>You will need the files from Assignment 0 so be sure to pull them over
to your new repo.</p>
<div class="toctree-wrapper compound">
<p class="caption"><span class="caption-text">Guides</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="tensordata.html">Tensors</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensorops.html">Operations</a></li>
<li class="toctree-l1"><a class="reference internal" href="tensor.html">Tensor Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="broadcasting.html">Broadcasting</a></li>
</ul>
</div>
<div class="section" id="tasks">
<h2>Tasks<a class="headerlink" href="#tasks" title="Permalink to this headline">¶</a></h2>
<div class="section" id="tasks-2-1-tensor-data-indexing">
<h3>Tasks 2.1: Tensor Data - Indexing<a class="headerlink" href="#tasks-2-1-tensor-data-indexing" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with tensor indexing.
Be sure to first carefully read the section on
<a class="reference internal" href="tensordata.html"><span class="doc">Tensors</span></a>. You may also find it helpful to read
the tutorials for using tensors/arrays in Torch or Numpy.</p>
</div>
<p>The MiniTorch library implements the core multidimensional tensor backend as
<code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.TensorData</span></code>. This is class handles indexing, storage, transposition,
and low-level details such as strides. Before turning to the user-facing class <code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Tensor</span></code>,
first implement these core functions.</p>
<div class="admonition-todo admonition" id="id1">
<p class="admonition-title">Todo</p>
<p>Add calls in <cite>minitorch/tensor_data.py</cite> for each of the following, and pass tests marked as <cite>task2_1</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.index_to_position">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">index_to_position</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.index_to_position" title="Permalink to this definition">¶</a></dt>
<dd><p>Unmaterialized device function</p>
</dd></dl>

<dl class="py function">
<dt id="minitorch.count">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">count</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.count" title="Permalink to this definition">¶</a></dt>
<dd><p>Unmaterialized device function</p>
</dd></dl>

<dl class="py function">
<dt id="minitorch.TensorData.permute">
<code class="sig-prename descclassname">minitorch.TensorData.</code><code class="sig-name descname">permute</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">self</span></em>, <em class="sig-param"><span class="o">*</span><span class="n">order</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.TensorData.permute" title="Permalink to this definition">¶</a></dt>
<dd><p>Permute the dimensions of the tensor.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>order</strong> (<em>list</em>) -- a permutation of the dimensions</p>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>a new TensorData with the same storage and a new dimension order.</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p><code class="xref py py-class docutils literal notranslate"><span class="pre">TensorData</span></code></p>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="tasks-2-2-tensor-operations">
<h3>Tasks 2.2: Tensor Operations<a class="headerlink" href="#tasks-2-2-tensor-operations" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with higher-order tensor operations.
Be sure to first carefully read the section on
<a class="reference internal" href="tensorops.html"><span class="doc">Operations</span></a>. You may also find it helpful to go back to
Module 0 and make sure you understand higher-order functions and currying.</p>
</div>
<p>The tensor operations will to apply high-level, higher-order
operations to all values in a tensor simultaneously. In particularly
allowing you to map, zip, and reduce tensor data objects together. On
top of this foundation we can build up a similar class to the Tensor functions
just like we did for the scalar functions. In this task you should first implement
the generic tensor ops and then use these to provide the <cite>forward</cite> functions
for tensors.</p>
<span class="target" id="module-minitorch.tensor_ops"></span><div class="admonition-todo admonition" id="id2">
<p class="admonition-title">Todo</p>
<p>Add functions in <cite>minitorch/tensor_ops.py</cite> and <cite>minitorch/tensor.py</cite> for each of the following, and pass tests marked as <cite>task2_2</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.tensor_map">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">tensor_map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor_map" title="Permalink to this definition">¶</a></dt>
<dd><p>Higher-order tensor map function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mappings floats-to-floats to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for out tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for out tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for out tensor.</p></li>
<li><p><strong>out_size</strong> (<em>int</em>) -- size of out</p></li>
<li><p><strong>in_storage</strong> (<em>array</em>) -- storage for in tensor.</p></li>
<li><p><strong>in_shape</strong> (<em>array</em>) -- shape for in tensor.</p></li>
<li><p><strong>in_strides</strong> (<em>array</em>) -- strides for in tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.tensor_zip">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">tensor_zip</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor_zip" title="Permalink to this definition">¶</a></dt>
<dd><p>Higher-order tensor zipWith (or map2) function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mappings two floats to float to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_size</strong> (<em>int</em>) -- size of <cite>out</cite>  tensor</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>b_storage</strong> (<em>array</em>) -- storage for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_shape</strong> (<em>array</em>) -- shape for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_strides</strong> (<em>array</em>) -- strides for <cite>b</cite> tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.tensor_reduce">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">tensor_reduce</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.tensor_reduce" title="Permalink to this definition">¶</a></dt>
<dd><p>Higher-order tensor reduce function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mapping two floats to float for combine.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_size</strong> (<em>int</em>) -- size of <cite>out</cite> tensor</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>reduce_shape</strong> (<em>array</em>) -- shape of reduction (1 for dimension kept, shape value for dimensions summed out)</p></li>
<li><p><strong>reduce_size</strong> (<em>int</em>) -- size of reduce shape</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="tasks-2-3-gradients-and-autograd">
<h3>Tasks 2.3: Gradients and Autograd<a class="headerlink" href="#tasks-2-3-gradients-and-autograd" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with tensor backward operations.
Be sure to first carefully read the section on
<a class="reference internal" href="tensor.html"><span class="doc">Tensor Variables</span></a>. You may also find it helpful to go back to
Module 1 and review <cite>variables</cite> and <cite>functions</cite>.</p>
</div>
<p>Just as with <a class="reference internal" href="scalar.html#minitorch.Scalar" title="minitorch.Scalar"><code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Scalar</span></code></a>, the <code class="xref py py-class docutils literal notranslate"><span class="pre">minitorch.Tensor</span></code>
is a Variable that support automatic differentiation. In this task you
will need to implement each of the backward functions and ensure that
they pass the tests.</p>
<div class="admonition-todo admonition" id="id3">
<p class="admonition-title">Todo</p>
<p>Add backward functions <cite>minitorch/tensor_ops.py</cite> for each of the following, and pass tests marked as <cite>task2_3</cite>.</p>
</div>
</div>
<div class="section" id="tasks-2-4-tensor-broadcasting">
<h3>Tasks 2.4: Tensor Broadcasting<a class="headerlink" href="#tasks-2-4-tensor-broadcasting" title="Permalink to this headline">¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This task requires familiarity with tensor broadcasting.  Be
sure to first carefully read the section on
<a class="reference internal" href="broadcasting.html"><span class="doc">Broadcasting</span></a>. You may also find it helpful to go through
some of the tutorials of Torch or Numpy broadcasting as it is
identical.</p>
</div>
<div class="admonition-todo admonition" id="id4">
<p class="admonition-title">Todo</p>
<p>Add broadcasing functions to <cite>minitorch/tensor_data.py</cite> and <cite>minitorch/tensor_ops.py</cite> for each of the following, and pass tests marked as <cite>task2_4</cite>.</p>
</div>
<dl class="py function">
<dt id="minitorch.shape_broadcast">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">shape_broadcast</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">shape1</span></em>, <em class="sig-param"><span class="n">shape2</span></em><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.shape_broadcast" title="Permalink to this definition">¶</a></dt>
<dd><p>Broadcast two shapes to create a new union shape.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>shape1</strong> (<em>tuple</em>) -- first shape</p></li>
<li><p><strong>shape2</strong> (<em>tuple</em>) -- second shape</p></li>
</ul>
</dd>
<dt class="field-even">Returns</dt>
<dd class="field-even"><p>broadcasted shape</p>
</dd>
<dt class="field-odd">Return type</dt>
<dd class="field-odd"><p>tuple</p>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="minitorch.broadcast_index">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">broadcast_index</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#minitorch.broadcast_index" title="Permalink to this definition">¶</a></dt>
<dd><p>Unmaterialized device function</p>
</dd></dl>

<p>You will also need to revistit these functions to make sure that broadcast index is used.</p>
<dl class="py function">
<dt id="id0">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">tensor_map</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#id0" title="Permalink to this definition">¶</a></dt>
<dd><p>Higher-order tensor map function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mappings floats-to-floats to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for out tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for out tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for out tensor.</p></li>
<li><p><strong>out_size</strong> (<em>int</em>) -- size of out</p></li>
<li><p><strong>in_storage</strong> (<em>array</em>) -- storage for in tensor.</p></li>
<li><p><strong>in_shape</strong> (<em>array</em>) -- shape for in tensor.</p></li>
<li><p><strong>in_strides</strong> (<em>array</em>) -- strides for in tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="id5">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">tensor_zip</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#id5" title="Permalink to this definition">¶</a></dt>
<dd><p>Higher-order tensor zipWith (or map2) function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mappings two floats to float to apply.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_size</strong> (<em>int</em>) -- size of <cite>out</cite>  tensor</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>b_storage</strong> (<em>array</em>) -- storage for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_shape</strong> (<em>array</em>) -- shape for <cite>b</cite> tensor.</p></li>
<li><p><strong>b_strides</strong> (<em>array</em>) -- strides for <cite>b</cite> tensor.</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<dl class="py function">
<dt id="id6">
<code class="sig-prename descclassname">minitorch.</code><code class="sig-name descname">tensor_reduce</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">fn</span></em><span class="sig-paren">)</span><a class="headerlink" href="#id6" title="Permalink to this definition">¶</a></dt>
<dd><p>Higher-order tensor reduce function.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>fn</strong> -- function mapping two floats to float for combine.</p></li>
<li><p><strong>out</strong> (<em>array</em>) -- storage for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_shape</strong> (<em>array</em>) -- shape for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_strides</strong> (<em>array</em>) -- strides for <cite>out</cite> tensor.</p></li>
<li><p><strong>out_size</strong> (<em>int</em>) -- size of <cite>out</cite> tensor</p></li>
<li><p><strong>a_storage</strong> (<em>array</em>) -- storage for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_shape</strong> (<em>array</em>) -- shape for <cite>a</cite> tensor.</p></li>
<li><p><strong>a_strides</strong> (<em>array</em>) -- strides for <cite>a</cite> tensor.</p></li>
<li><p><strong>reduce_shape</strong> (<em>array</em>) -- shape of reduction (1 for dimension kept, shape value for dimensions summed out)</p></li>
<li><p><strong>reduce_size</strong> (<em>int</em>) -- size of reduce shape</p></li>
</ul>
</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="task-2-5-training">
<h3>Task 2.5: Training<a class="headerlink" href="#task-2-5-training" title="Permalink to this headline">¶</a></h3>
<p>If your code works you should now be able to run the tensor training script in <cite>project/run_tensor.py</cite>.</p>
<div class="admonition-todo admonition" id="id7">
<p class="admonition-title">Todo</p>
<p>Train a tensor model and add you results to the README.</p>
</div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="tensordata.html" class="btn btn-neutral float-right" title="Tensors" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="backpropagate.html" class="btn btn-neutral float-left" title="Backpropagate" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, Sasha Rush

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>